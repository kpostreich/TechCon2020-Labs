{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Conference Event Link When: March 16\u201319, 2020 Where: Renaissance Hotel Dallas 2222 North Stemmons Freeway Dallas, TX 75207 Starts: Mar 16, 2020 1:00 PM (CT) Ends: Mar 19, 2020 5:30 PM (CT)","title":"Home"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/","text":"ITC-19 Evaluate Java Application with IBM Cloud Transformation Advisor Modernizing WebSphere / Java applications for Cloud and Containers Lab: Evaluate On-Premises Java Applications with IBM Cloud Transformation Advisor On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to cloud quickly and cost-effectively. The I BM Cloud Pak for Applications provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. One of the tools included in the Pak is the IBM Cloud Transformation Advisor ( Transformation Advisor ), a developer tool that is available at no charge to help you quickly evaluate on-premises Java EE applications for deployment to the cloud. The Transformation Advisor tool can identify the Java EE programming models in the app. determine the complexity of apps by listing a high-level inventory of the content and structure of each app. highlight Java EE programming model and WebSphere API differences between the WebSphere profile types learn any Java EE specification implementation differences that might affect the app Additionally, the tool provides a recommendation for the right-fit IBM WebSphere Application Server edition and offers advice, best practices and potential solutions to assess the ease of moving apps to Liberty or newer versions of WebSphere traditional. It will accelerate application migrating to cloud process, minimize errors and risks and reduce time to market. This lab is a part of the Application Modernization lab series which focus on the evaluation, re-platform, repackage, and refactor modernization approaches and solutions. This lab covers the evaluation process. It will show the value of using Transformation Advisor to evaluate on-premises Java applications and identify a migration candidate for moving to the cloud. When you complete this lab, you will learn how to use this tool to quickly analyze on-premise Java applications without accessing their source code and to estimate the move to cloud efforts. Business Scenario As shown in the image below, your company has several web applications deployed to WebSphere Application Server ( WAS ) environment. Your company wants to move these applications to a lightweight WebSphere Liberty server on cloud, but you are not sure how much effort the migration process might take. You decide to use the Transformation Advisor to do a quick evaluation of these apps without their source code (Binary Scan using TA) to identify a good candidate application to move to cloud based on the analysis results. Objective The objectives of this lab are to: learn how to collect Java application and configuration data using the Transformation Advisor Data Collector tool. learn how to use the Transformation Advisor to evaluate the move to cloud efforts and to identify the good candidate for migration. Prerequisites The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands Have internet access Have basic Java app development knowledge. The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information. What is Already Completed Four (4) Linux VMs have been provided for this lab. The OpenShift Container Platform, OCP v4.2 , included with Cloud Pak for Applications , is installed in four VMs, the master1 VM, the dns VM, the worker1 VM, with one master node and one compute node. A local version of IBM Cloud Transformation Advisor , running on Docker container is used for this lab. The Transformation Advisor can be deployed to the RHOCP cluster as a part of IBM Cloud Pak for Applications installation. It also can be installed as a stand-alone local application. For more information on how to install a local version of the Transformation Advisor , please visit: https://www.ibm.com/cloud/architecture/tutorials/install-ibm-transformation-advisor-local The CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/cp4a-labs/lab1 directory of the Workstation VM. You can use this to copy / paste these commands to the Terminal window during this lab. The Workstation VM is the one you will use to access and work with the OCP cluster in this lab. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd Lab Tasks In this lab, you access WebSphere Application Server to review the deployment of the JEE applications. Then you are going to the Transformation Advisor to identify a good candidate application for moving to cloud. To identify which Java EE programming models are on the server, you could run the Transformation Advisor Data Collector tool against the server. The Transformation Advisor creates an inventory of the content and structure of each app and learn about problems that might occur if you move the app to cloud. Finally, you review the analysis reports to determine the complexity of the move-to-cloud efforts and select the migration candidate app. Here are the activities involved in this process: Log in to WebSphere Application Server to review the deployed JEE applications Run the Transformation Advisor Data Collector tool against the WebSphere Application Server to get application data Review the analysis reports that Transformation Advisor generates to identify the right candidate application for a rapid and cost-effective migration to cloud Execute Lab Tasks To get started, login to the environment Log in to the Workstation VM and Get Started a) If he VMs are not already started, start them by clicking the Play button b) After the VMs are started, click the Workstation VM icon to access it. The Workstation Linux Desktop is displayed. You will execute all the lab tasks on this VM. Review the on-prem WebSphere apps In this task, you will take a look at the sample applications deployed to the local WebSphere Application Server (WAS) environment. You are going to identify one of them to be the god candidate to move the cloud. Start WebSphere Application Server, which is running on the Workstation VM. The DB2 database it accesses runs in a Docker container on the VM. In the Workstation VM, we have a local WebSphere Application Server which hosts several sample applications. To start the WAS server: a) Open a terminal window by clicking its icon on the Workstation VM desktop tool bar. b) In the terminal window, issue the command below to start the WAS server (You can copy / paste the command from the Commands.txt file located at /home/ibmdemo/cp4a-labs/lab1 /home/ibmdemo/cp4a-labs/shared/startWAS.sh when prompted, enter the sudo user password as: passw0rd . Within a couple of minutes, the WAS server will be ready. Access the WAS Admin Console to view the application deployed by clicking the web browser icon desktop tool bar to open a browser window. a) From the Web browser , click WebSphere Integrated Solution Console bookmark to launch the WAS console. b) Login to the WAS Admin Console login page, using the credentials below: User: wsadmin Password: passw0rd c) On the WAS Console page, click Applications -> Application Types -> WebSphere enterprise applications to view the apps deployed. In the Enterprise Applications list, you can see all applications deployed. Next we will use Transformation Advisor to analyze these applications to identify a good candidate to be moved to the cloud. Launch Transformation Advisor The Transformation Advisor can evaluate any Java based applications. In this lab, you are going to use it to evaluate whether the on-premises WebSphere application, Mod Resorts , is suitable to move to cloud and what the effort might be to get it there. The Transformation Advisor is installed locally on the Workstation VM. Launch the Transformation Advisor tool using the steps below. From Workstation VM Desktop Tool Bar, click the Terminal icon to open a Terminal window. Launch the Transformation Advisor with command: /home/ibmdemo/ta-local/transformationAdvisor/launchTransformationAdvisor.sh Type 5 and press Enter to start the Transformation Advisor . The Transformation Advisor application is started, right-click the application URL link and select Open Link to launch it in a web browser window. The Transformation Advisor Home page is displayed. Download Transformation Advisor Data Collector utility Now the Transformation Advisor is running, you will use its Data Collector utility to get the application data from WebSphere Application Server running on the Workstation VM. To evaluate on-premises Java applications, you need to run Transformation Advisor Data Collector utility against the Application server environment. It will extract all application information from the environment. The utility can be downloaded from the Transformation Advisor web page. From the Transformation Advisor Home page, Add a new workspace by entering the workspace name as Evaluation and then clicking Next . A workspace is a designated area that will house the migration recommendations provided by Transformation Advisor from your application server environment. You can name and organize these however you want, whether it\u2019s by business application, location, or teams Enter the collection name as Server1 and click Let\u2019s go . Each workspace can be divided into collections for more focused assessment and planning. Like workspaces, collections can be named and organized in whatever way you want. Once the Workspace and Collection are created, you will have options to either download the Data Collector utility or upload existing data file. In this lab, we ae going to use the Data Collector utility to get the data from the local WebSphere environment. a) Click Data Collector to go to the download page. In the Download page, you can download different versions of the utility based on your source operating system. It also shows how to use the command line utility to collect application data from WebSphere, WebLogic and Tomcat servers. a) Since the lab VM is a Linux OS, click Download Linux to get the utility. b) In the Download dialog window, select the Save File option and click OK . The zipped Data Collector utility file will be saved in / home/ibmdemo/Downloads directory of Workstation VM. Run Transformation Advisor Data Collector utility After downloading the zipped Data Collector utility, we need to unpack it and run the utility against the WAS server to collect all the data of deployed applications and their configuration from the WAS server. Go back to the terminal window and navigate the /home/ibmdemo/Downloads directory and view its contents with commands: cd /home/ibmdemo/Downloads/ ls -l You can see the downloaded data collector utility file saved in the directory. Extract the data collector utility with commands: tar xvfz transformationadvisor-Linux\\_Evaluation\\_Server1.tgz The data collector utility will be extracted to /home/ibmdemo/Downloads /transformationadvisor-2.0.2 directory. Execute the Data Collector utility with the commands below to start collect the deployed applications information on the WAS server. cd /home/ibmdemo/Downloads*/transformationadvisor-2.0.2 sudo ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer -p AppSrv01 wsadmin passw0rd when prompted, enter the sudo password as: passw0rd Type 1 to accept the license agreement and press Enter , as illustrated below Note: The utility will start to collect application data. This process will take a few minutes to complete, depending on how many applications are deployed on the WAS server. In this lab, it might be a few minutes. When the collection utility competes, you will see a message \u201c Thank you for uploading your data. You can proceed to the application UI for doing further analysis.\u201d Your application data is collected, it is saved as a zip file in directory /home/ibmdemo/Downloads/transformationadvisor-2.0.2/AppSrv01.zip , as shown below. In general, if your application server and the Transformation Advisor are in the same network infrastructure, the collected data will be automatically uploaded to Transformation Advisor for you to view the analysis results. Otherwise, you must manually upload the data to Transformation Advisor before you can view the results. Evaluate On-Premises Java Applications In this section, we are going to use Transformation Advisor UI to view the application data analysis results. Go back to Transformation Advisor page in web browser, click the Recommendations link to the Recommendations page. In the Recommendations page, you can see all applications deployed to the WAS server are listed. On the Recommendations page, the identified migration source environment is shown in the Profile section, and the target environment is shown in the Preferred migration section. The data collector tool detects that the source environment is your WebSphere Application Server ND AppSrv01 profile. The target environment is Liberty on Private Cloud , which is the default target environment. The Recommendations page also shows the summary analysis results for all the apps in the AppSrv01 environment to be moved to a Liberty on Private Cloud environment. For each app, you can see these results: - Complexity level - Technology match - Dependencies - Issues - Estimated development cost in days For example, if you want to move the modresorts-1_0_war.ear application to Liberty on Private Cloud, the complexity level is Simple . The Tech match is 100% , which indicates that the application code does not need to be changed before it can be moved to cloud. The application has no dependency, has 1 minor level issue and the estimated development effort is 0 day. As you can see, the default move to cloud environment is Liberty on Private Cloud . However Transformation Advisor can also provide migration options if you want to migrate your application to different target environments. In this lab we are focusing on identifying a good candidate for moving to the Liberty on Private Cloud environment, so the default target environment will work for us. If you look at the complexities of these applications, you can see that the moderesorts-1.0_war.ear ( Mod Resorts app) and pbw-ear.ear ( PlansByWebSphere app) has the simple complexity, which mean that these two apps can be migrated to cloud without any code change. But since the moderesorts-1.0_war.ear app has fewer potential issues (1) than the pbw-ear.ear app (4). Let\u2019s look at the analysis results for moderesorts-1.0_war.ear application in detail. Click the modresorts-1_0_war.ear link to expand its analysis results to view the assessment and reports. The first section in the detail analysis summary page is the Overall Complexity section. The overall complexity for the application is simple , indicating that the application can be directly moved to cloud without any code change. a) Scroll down to Application Details section. You can see there is no code change required. The the estimate migration overhead development cost is 5 days. This estimate is based on data from IBM Services engagements,which includes migrating management, server configuration, and testing. Continue to scroll down to the Issues section. You can see the only minor potential issue listed is on configuring the application in Docker container. Next, scroll down to the bottom of the page and click the Technology Report link, this will open a new browser window to show the application Evaluation Report . The report lists all java technologies the application uses and whether these technologies are supported by a specific WebSphere platform from Liberty for Java on IBM Cloud to WebSphere traditional for z/OS. It is used to determine whether a particular WebSphere product is suitable for an application. As we can see from the report, the Mod Resorts application only uses Java Servlet which is supported by all WebSphere platforms. Go back to the Transformation Advisor page and click the Analysis Report link. a) Click OK to continue Now you see the Detailed Migration Analysis Report opened in a new browser window. This is the deep-dive report which shows all issue found at the code level. b) Scroll down to Detailed Results by Rule section, you can see all the java technology issues identified based on different migration rules. For the Mod Resorts application, there is one warning rule regarding the application configuration in Docker containers. c) Click the Show results link related to the Warning for configuration in Docker containers. You can see the detail analysis of the issue at code level , in a specific class file and specific line. This will help developers to pinpoint where the issue is, or potential issue may be. d) Click the Show rule help link. This will expand the Rule Help section which provides recommended solutions on how to fix the issue. For the Docker container configuration issue, the utility provides best practice suggestion to externalize the configuration for the container Go back to the Transformation Advisor page and click the Inventory Report link. The Inventory Report helps you examine what is in your application, including the number of modules, their relationships and the technologies in those modules. It also gives you a view of all the utility JAR files in the application that tend to accumulate over time. Potential deployment problems and performance considerations are also included. a) Scroll down to view this report which serves as good decision-making tool to inform you what is inside your app runtime. It helps provide a clearer understanding of the app runtime, the components it has and the relationships among them. From the analysis reports we looked at above, we know that the Mod Resorts application is supported by Liberty on Private Cloud as the target environment, and the issue that the tool identified would not affect the application migration. You can confidently select the application as a good candidate for moving to liberty on cloud in the repackage process with minimum effort. Now you know that the Mod Resorts application can be repacked to Liberty on cloud, you want to know if it is also a good candidate to re-platform with WAS Base on cloud. a) To do that, return to the TA recommendations page. Then change the target environment from Liberty on Private Cloud to WebSphere Traditional on Private Cloud . As you can see from the TA recommendation, the Mod Resorts application is also a good candidate for re-platform in WAS container on cloud, if desired. If you want to review the recommendation details, you can follow the same steps you did before to go over them. Quick Detour - Additional powerful capabilities in Transformation Advisor! How Transformation Advisor helps Modernize a Java EE application to IBM Cloud Pak for Applications on RedHat OpenShift** Transformation Advisor helps accelerate the development activities required during the modernization of a Java EE application currently deployed to IBM WebSphere Application Server, Oracle WebLogic, Apache Tomcat or Red Hat JBoss Application Server. The capabilities of Transformation Advisor help bring the cloud-native development experience to the modernization of existing WebSphere / Java applications. For reference, the cloud-native development process using Application Stacks simplifies the process of building and deploying applications to Kubernetes View the migration bundle contents for the ModResorts application Transformation Advisor generates migration artifacts that are needed to deploy a modernized application to IBM Cloud Pak for Applications and RedHat OpenShift. This section simply provides a quick peak into this exciting capability from Transformation Advisor. Additional hands-on labs on these capabilities can be arranged through your IBM representative. Click on the menu with the \u201chamburger icon\u201d next to the ModResorts application analysis to display the menu choices and choose View migration plan : Transformation Advisor will display details about the migration bundle that it generated to accelerate the modernization of this application into IBM Cloud Pak for Applications. The migration bundle includes diverse artifacts, depending on the needs of the application. As illustrated below, the user can choose to create a migration bundle for either: - a binary project of an application (uploading a WAR/EAR file and its dependent libraries) - a source project of an application, so that the application source files can be modernized and maintained. Notice the contents of the migration bundle. ( Migration Files ) The artifacts change, depending on the user\u2019s modernization choices for the application (Source or Binary). The default selection for an application that uses only EE7 features and above will be to deploy as a source application project . The project will be a source project, with the directory structure set up with a starter application and the Liberty configuration for the application. To accelerate the application modernization, the artifacts produced by Transformation Advisor include the configuration for the Liberty server, the build file for the application which can used with the Application Stacks, Dockerfile and Operator files for containerized deployments to RedHat OpenShift with IBM Cloud Pak for Applications. The user will be able to choose to download the artifacts as a migration bundle or push the bundle contents into a GitHub repository. See the illustration below. A complete exploration of these exciting capabilities is beyond the scope of this lab. However, these powerful features avaiable in Transformation Advisor help expedite the journey to cloud for existing WebSphere Applications. Evaluate the PlantsByWebSphere Application You have evaluated and assessed modernizing a simple application that uses only a single Java EE technology. Now, let\u2019s explore a more sophisticated Java EE application that was developed using many Java EE API's found in traditional WebSphere Applications. Navigate back to the Transformation Advisor Recommendations page. You can follow the same procedure you did for the Mod Resorts application to view the analysis results for the PlantsByWebSphere application. As you can see from the Summary list, the recommendations for the application to move to cloud are as follows: - The complexity level is Simple. - Tech match is 100%, which means that the application code can be deployed to Liberty on Private Cloud without any changes. - The app has 3 dependencies and 7 issues, including 1 high severity level, 1 medium severity level and 5 low severity level. - The estimated development effort is 0 day because no code changes are needed. View the Application Technology report for PlantsByWebSphereEE6.ear application. Notice all of the technologies that this application uses, all of which are suported in ALL of the editions of WebSPhere. Evaluate the Complex DefaultAppliction Application Navigate back to the Transformation Advisor Recommendations page. Select the DefaultApplication.ear application. As you can see from the Summary list, this is a more complex application to modernize. View the Application Technology report for DefaultApplication.ear application. As you review the Technology report, you will discover that the application makes extensive use of EJB Entity Beans. EJB Entity beans are a deprecated Java EE technology, and not supported in WebSphere Liberty, which is the preferred WebSphere edition for Cloud environments. The modernization of this application is more complex due to the need to rewrite parts of the application with Java EE technologies that are supported on WebSphere Liberty. However, WAS Base in containers is an alternative for running the application in IBM Cloud Pak for Applications with RedHat OpenShift. This option would allow you to move the application to cloud in containers (IBM Cloud Pak for Applications with Redhat OpenShift) with little or NO code changes. As the technology report reveals, the EJB Entity Beans technologies are supported in the traditional WAS Base and WAS ND editions. WAS Base in containers is a supported configuration in IB Cloud Pak for Applications. So, for this application, you might choose to move the application to WAS Base in containers, on cloud. Summary In this lab, you learned how to evaluate the existing Java application using IBM Transformation Advisor . As a part of IBM App Modernization solutions in IBM Cloud Pak for Applications , the Transformation Advisor tool provides a recommendation for the right-fit IBM WebSphere Application Server edition and offers advice, best practices and potential solutions to assess the ease of moving apps to Liberty or to WAS container, or to newer versions of WebSphere traditional. It will accelerate application migrating to cloud process, minimize errors and risks and reduce time to market. To learn more about IBM App Modernization solutions, please visit Cloud Pak for Applications . Congratulations! You have successfully completed the lab \u201cEvaluate On-Premises Java Apps with Transformation Advisor\u201d.","title":"**ITC-19**"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#itc-19","text":"","title":"ITC-19"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#evaluate-java-application-with-ibm-cloud-transformation-advisor","text":"Modernizing WebSphere / Java applications for Cloud and Containers","title":"Evaluate Java Application with IBM Cloud Transformation Advisor"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#lab-evaluate-on-premises-java-applications-with-ibm-cloud-transformation-advisor","text":"On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to cloud quickly and cost-effectively. The I BM Cloud Pak for Applications provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. One of the tools included in the Pak is the IBM Cloud Transformation Advisor ( Transformation Advisor ), a developer tool that is available at no charge to help you quickly evaluate on-premises Java EE applications for deployment to the cloud. The Transformation Advisor tool can identify the Java EE programming models in the app. determine the complexity of apps by listing a high-level inventory of the content and structure of each app. highlight Java EE programming model and WebSphere API differences between the WebSphere profile types learn any Java EE specification implementation differences that might affect the app Additionally, the tool provides a recommendation for the right-fit IBM WebSphere Application Server edition and offers advice, best practices and potential solutions to assess the ease of moving apps to Liberty or newer versions of WebSphere traditional. It will accelerate application migrating to cloud process, minimize errors and risks and reduce time to market. This lab is a part of the Application Modernization lab series which focus on the evaluation, re-platform, repackage, and refactor modernization approaches and solutions. This lab covers the evaluation process. It will show the value of using Transformation Advisor to evaluate on-premises Java applications and identify a migration candidate for moving to the cloud. When you complete this lab, you will learn how to use this tool to quickly analyze on-premise Java applications without accessing their source code and to estimate the move to cloud efforts.","title":"Lab:  Evaluate On-Premises Java Applications with IBM Cloud Transformation Advisor"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#business-scenario","text":"As shown in the image below, your company has several web applications deployed to WebSphere Application Server ( WAS ) environment. Your company wants to move these applications to a lightweight WebSphere Liberty server on cloud, but you are not sure how much effort the migration process might take. You decide to use the Transformation Advisor to do a quick evaluation of these apps without their source code (Binary Scan using TA) to identify a good candidate application to move to cloud based on the analysis results.","title":"Business Scenario"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#objective","text":"The objectives of this lab are to: learn how to collect Java application and configuration data using the Transformation Advisor Data Collector tool. learn how to use the Transformation Advisor to evaluate the move to cloud efforts and to identify the good candidate for migration.","title":"Objective"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#prerequisites","text":"The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands Have internet access Have basic Java app development knowledge. The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information.","title":"Prerequisites"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#what-is-already-completed","text":"Four (4) Linux VMs have been provided for this lab. The OpenShift Container Platform, OCP v4.2 , included with Cloud Pak for Applications , is installed in four VMs, the master1 VM, the dns VM, the worker1 VM, with one master node and one compute node. A local version of IBM Cloud Transformation Advisor , running on Docker container is used for this lab. The Transformation Advisor can be deployed to the RHOCP cluster as a part of IBM Cloud Pak for Applications installation. It also can be installed as a stand-alone local application. For more information on how to install a local version of the Transformation Advisor , please visit: https://www.ibm.com/cloud/architecture/tutorials/install-ibm-transformation-advisor-local The CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/cp4a-labs/lab1 directory of the Workstation VM. You can use this to copy / paste these commands to the Terminal window during this lab. The Workstation VM is the one you will use to access and work with the OCP cluster in this lab. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd","title":"What is Already Completed"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#lab-tasks","text":"In this lab, you access WebSphere Application Server to review the deployment of the JEE applications. Then you are going to the Transformation Advisor to identify a good candidate application for moving to cloud. To identify which Java EE programming models are on the server, you could run the Transformation Advisor Data Collector tool against the server. The Transformation Advisor creates an inventory of the content and structure of each app and learn about problems that might occur if you move the app to cloud. Finally, you review the analysis reports to determine the complexity of the move-to-cloud efforts and select the migration candidate app. Here are the activities involved in this process: Log in to WebSphere Application Server to review the deployed JEE applications Run the Transformation Advisor Data Collector tool against the WebSphere Application Server to get application data Review the analysis reports that Transformation Advisor generates to identify the right candidate application for a rapid and cost-effective migration to cloud","title":"Lab Tasks"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#execute-lab-tasks","text":"","title":"Execute Lab Tasks"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#to-get-started-login-to-the-environment","text":"Log in to the Workstation VM and Get Started a) If he VMs are not already started, start them by clicking the Play button b) After the VMs are started, click the Workstation VM icon to access it. The Workstation Linux Desktop is displayed. You will execute all the lab tasks on this VM.","title":"To get started, login to the environment"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#review-the-on-prem-websphere-apps","text":"In this task, you will take a look at the sample applications deployed to the local WebSphere Application Server (WAS) environment. You are going to identify one of them to be the god candidate to move the cloud. Start WebSphere Application Server, which is running on the Workstation VM. The DB2 database it accesses runs in a Docker container on the VM. In the Workstation VM, we have a local WebSphere Application Server which hosts several sample applications. To start the WAS server: a) Open a terminal window by clicking its icon on the Workstation VM desktop tool bar. b) In the terminal window, issue the command below to start the WAS server (You can copy / paste the command from the Commands.txt file located at /home/ibmdemo/cp4a-labs/lab1 /home/ibmdemo/cp4a-labs/shared/startWAS.sh when prompted, enter the sudo user password as: passw0rd . Within a couple of minutes, the WAS server will be ready. Access the WAS Admin Console to view the application deployed by clicking the web browser icon desktop tool bar to open a browser window. a) From the Web browser , click WebSphere Integrated Solution Console bookmark to launch the WAS console. b) Login to the WAS Admin Console login page, using the credentials below: User: wsadmin Password: passw0rd c) On the WAS Console page, click Applications -> Application Types -> WebSphere enterprise applications to view the apps deployed. In the Enterprise Applications list, you can see all applications deployed. Next we will use Transformation Advisor to analyze these applications to identify a good candidate to be moved to the cloud.","title":"Review the on-prem WebSphere apps"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#launch-transformation-advisor","text":"The Transformation Advisor can evaluate any Java based applications. In this lab, you are going to use it to evaluate whether the on-premises WebSphere application, Mod Resorts , is suitable to move to cloud and what the effort might be to get it there. The Transformation Advisor is installed locally on the Workstation VM. Launch the Transformation Advisor tool using the steps below. From Workstation VM Desktop Tool Bar, click the Terminal icon to open a Terminal window. Launch the Transformation Advisor with command: /home/ibmdemo/ta-local/transformationAdvisor/launchTransformationAdvisor.sh Type 5 and press Enter to start the Transformation Advisor . The Transformation Advisor application is started, right-click the application URL link and select Open Link to launch it in a web browser window. The Transformation Advisor Home page is displayed.","title":"Launch Transformation Advisor"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#download-transformation-advisor-data-collector-utility","text":"Now the Transformation Advisor is running, you will use its Data Collector utility to get the application data from WebSphere Application Server running on the Workstation VM. To evaluate on-premises Java applications, you need to run Transformation Advisor Data Collector utility against the Application server environment. It will extract all application information from the environment. The utility can be downloaded from the Transformation Advisor web page. From the Transformation Advisor Home page, Add a new workspace by entering the workspace name as Evaluation and then clicking Next . A workspace is a designated area that will house the migration recommendations provided by Transformation Advisor from your application server environment. You can name and organize these however you want, whether it\u2019s by business application, location, or teams Enter the collection name as Server1 and click Let\u2019s go . Each workspace can be divided into collections for more focused assessment and planning. Like workspaces, collections can be named and organized in whatever way you want. Once the Workspace and Collection are created, you will have options to either download the Data Collector utility or upload existing data file. In this lab, we ae going to use the Data Collector utility to get the data from the local WebSphere environment. a) Click Data Collector to go to the download page. In the Download page, you can download different versions of the utility based on your source operating system. It also shows how to use the command line utility to collect application data from WebSphere, WebLogic and Tomcat servers. a) Since the lab VM is a Linux OS, click Download Linux to get the utility. b) In the Download dialog window, select the Save File option and click OK . The zipped Data Collector utility file will be saved in / home/ibmdemo/Downloads directory of Workstation VM.","title":"Download Transformation Advisor Data Collector utility"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#run-transformation-advisor-data-collector-utility","text":"After downloading the zipped Data Collector utility, we need to unpack it and run the utility against the WAS server to collect all the data of deployed applications and their configuration from the WAS server. Go back to the terminal window and navigate the /home/ibmdemo/Downloads directory and view its contents with commands: cd /home/ibmdemo/Downloads/ ls -l You can see the downloaded data collector utility file saved in the directory. Extract the data collector utility with commands: tar xvfz transformationadvisor-Linux\\_Evaluation\\_Server1.tgz The data collector utility will be extracted to /home/ibmdemo/Downloads /transformationadvisor-2.0.2 directory. Execute the Data Collector utility with the commands below to start collect the deployed applications information on the WAS server. cd /home/ibmdemo/Downloads*/transformationadvisor-2.0.2 sudo ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer -p AppSrv01 wsadmin passw0rd when prompted, enter the sudo password as: passw0rd Type 1 to accept the license agreement and press Enter , as illustrated below Note: The utility will start to collect application data. This process will take a few minutes to complete, depending on how many applications are deployed on the WAS server. In this lab, it might be a few minutes. When the collection utility competes, you will see a message \u201c Thank you for uploading your data. You can proceed to the application UI for doing further analysis.\u201d Your application data is collected, it is saved as a zip file in directory /home/ibmdemo/Downloads/transformationadvisor-2.0.2/AppSrv01.zip , as shown below. In general, if your application server and the Transformation Advisor are in the same network infrastructure, the collected data will be automatically uploaded to Transformation Advisor for you to view the analysis results. Otherwise, you must manually upload the data to Transformation Advisor before you can view the results.","title":"Run Transformation Advisor Data Collector utility"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#evaluate-on-premises-java-applications","text":"In this section, we are going to use Transformation Advisor UI to view the application data analysis results. Go back to Transformation Advisor page in web browser, click the Recommendations link to the Recommendations page. In the Recommendations page, you can see all applications deployed to the WAS server are listed. On the Recommendations page, the identified migration source environment is shown in the Profile section, and the target environment is shown in the Preferred migration section. The data collector tool detects that the source environment is your WebSphere Application Server ND AppSrv01 profile. The target environment is Liberty on Private Cloud , which is the default target environment. The Recommendations page also shows the summary analysis results for all the apps in the AppSrv01 environment to be moved to a Liberty on Private Cloud environment. For each app, you can see these results: - Complexity level - Technology match - Dependencies - Issues - Estimated development cost in days For example, if you want to move the modresorts-1_0_war.ear application to Liberty on Private Cloud, the complexity level is Simple . The Tech match is 100% , which indicates that the application code does not need to be changed before it can be moved to cloud. The application has no dependency, has 1 minor level issue and the estimated development effort is 0 day. As you can see, the default move to cloud environment is Liberty on Private Cloud . However Transformation Advisor can also provide migration options if you want to migrate your application to different target environments. In this lab we are focusing on identifying a good candidate for moving to the Liberty on Private Cloud environment, so the default target environment will work for us. If you look at the complexities of these applications, you can see that the moderesorts-1.0_war.ear ( Mod Resorts app) and pbw-ear.ear ( PlansByWebSphere app) has the simple complexity, which mean that these two apps can be migrated to cloud without any code change. But since the moderesorts-1.0_war.ear app has fewer potential issues (1) than the pbw-ear.ear app (4). Let\u2019s look at the analysis results for moderesorts-1.0_war.ear application in detail. Click the modresorts-1_0_war.ear link to expand its analysis results to view the assessment and reports. The first section in the detail analysis summary page is the Overall Complexity section. The overall complexity for the application is simple , indicating that the application can be directly moved to cloud without any code change. a) Scroll down to Application Details section. You can see there is no code change required. The the estimate migration overhead development cost is 5 days. This estimate is based on data from IBM Services engagements,which includes migrating management, server configuration, and testing. Continue to scroll down to the Issues section. You can see the only minor potential issue listed is on configuring the application in Docker container. Next, scroll down to the bottom of the page and click the Technology Report link, this will open a new browser window to show the application Evaluation Report . The report lists all java technologies the application uses and whether these technologies are supported by a specific WebSphere platform from Liberty for Java on IBM Cloud to WebSphere traditional for z/OS. It is used to determine whether a particular WebSphere product is suitable for an application. As we can see from the report, the Mod Resorts application only uses Java Servlet which is supported by all WebSphere platforms. Go back to the Transformation Advisor page and click the Analysis Report link. a) Click OK to continue Now you see the Detailed Migration Analysis Report opened in a new browser window. This is the deep-dive report which shows all issue found at the code level. b) Scroll down to Detailed Results by Rule section, you can see all the java technology issues identified based on different migration rules. For the Mod Resorts application, there is one warning rule regarding the application configuration in Docker containers. c) Click the Show results link related to the Warning for configuration in Docker containers. You can see the detail analysis of the issue at code level , in a specific class file and specific line. This will help developers to pinpoint where the issue is, or potential issue may be. d) Click the Show rule help link. This will expand the Rule Help section which provides recommended solutions on how to fix the issue. For the Docker container configuration issue, the utility provides best practice suggestion to externalize the configuration for the container Go back to the Transformation Advisor page and click the Inventory Report link. The Inventory Report helps you examine what is in your application, including the number of modules, their relationships and the technologies in those modules. It also gives you a view of all the utility JAR files in the application that tend to accumulate over time. Potential deployment problems and performance considerations are also included. a) Scroll down to view this report which serves as good decision-making tool to inform you what is inside your app runtime. It helps provide a clearer understanding of the app runtime, the components it has and the relationships among them. From the analysis reports we looked at above, we know that the Mod Resorts application is supported by Liberty on Private Cloud as the target environment, and the issue that the tool identified would not affect the application migration. You can confidently select the application as a good candidate for moving to liberty on cloud in the repackage process with minimum effort. Now you know that the Mod Resorts application can be repacked to Liberty on cloud, you want to know if it is also a good candidate to re-platform with WAS Base on cloud. a) To do that, return to the TA recommendations page. Then change the target environment from Liberty on Private Cloud to WebSphere Traditional on Private Cloud . As you can see from the TA recommendation, the Mod Resorts application is also a good candidate for re-platform in WAS container on cloud, if desired. If you want to review the recommendation details, you can follow the same steps you did before to go over them.","title":"Evaluate On-Premises Java Applications"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#quick-detour-additional-powerful-capabilities-in-transformation-advisor","text":"","title":"Quick Detour - Additional powerful capabilities in Transformation Advisor!"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#how-transformation-advisor-helps-modernize-a-java-ee-application-to-ibm-cloud-pak-for-applications-on-redhat-openshift","text":"Transformation Advisor helps accelerate the development activities required during the modernization of a Java EE application currently deployed to IBM WebSphere Application Server, Oracle WebLogic, Apache Tomcat or Red Hat JBoss Application Server. The capabilities of Transformation Advisor help bring the cloud-native development experience to the modernization of existing WebSphere / Java applications. For reference, the cloud-native development process using Application Stacks simplifies the process of building and deploying applications to Kubernetes View the migration bundle contents for the ModResorts application Transformation Advisor generates migration artifacts that are needed to deploy a modernized application to IBM Cloud Pak for Applications and RedHat OpenShift. This section simply provides a quick peak into this exciting capability from Transformation Advisor. Additional hands-on labs on these capabilities can be arranged through your IBM representative. Click on the menu with the \u201chamburger icon\u201d next to the ModResorts application analysis to display the menu choices and choose View migration plan : Transformation Advisor will display details about the migration bundle that it generated to accelerate the modernization of this application into IBM Cloud Pak for Applications. The migration bundle includes diverse artifacts, depending on the needs of the application. As illustrated below, the user can choose to create a migration bundle for either: - a binary project of an application (uploading a WAR/EAR file and its dependent libraries) - a source project of an application, so that the application source files can be modernized and maintained. Notice the contents of the migration bundle. ( Migration Files ) The artifacts change, depending on the user\u2019s modernization choices for the application (Source or Binary). The default selection for an application that uses only EE7 features and above will be to deploy as a source application project . The project will be a source project, with the directory structure set up with a starter application and the Liberty configuration for the application. To accelerate the application modernization, the artifacts produced by Transformation Advisor include the configuration for the Liberty server, the build file for the application which can used with the Application Stacks, Dockerfile and Operator files for containerized deployments to RedHat OpenShift with IBM Cloud Pak for Applications. The user will be able to choose to download the artifacts as a migration bundle or push the bundle contents into a GitHub repository. See the illustration below. A complete exploration of these exciting capabilities is beyond the scope of this lab. However, these powerful features avaiable in Transformation Advisor help expedite the journey to cloud for existing WebSphere Applications.","title":"How Transformation Advisor helps Modernize a Java EE application to IBM Cloud Pak for Applications on RedHat OpenShift**"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#evaluate-the-plantsbywebsphere-application","text":"You have evaluated and assessed modernizing a simple application that uses only a single Java EE technology. Now, let\u2019s explore a more sophisticated Java EE application that was developed using many Java EE API's found in traditional WebSphere Applications. Navigate back to the Transformation Advisor Recommendations page. You can follow the same procedure you did for the Mod Resorts application to view the analysis results for the PlantsByWebSphere application. As you can see from the Summary list, the recommendations for the application to move to cloud are as follows: - The complexity level is Simple. - Tech match is 100%, which means that the application code can be deployed to Liberty on Private Cloud without any changes. - The app has 3 dependencies and 7 issues, including 1 high severity level, 1 medium severity level and 5 low severity level. - The estimated development effort is 0 day because no code changes are needed. View the Application Technology report for PlantsByWebSphereEE6.ear application. Notice all of the technologies that this application uses, all of which are suported in ALL of the editions of WebSPhere.","title":"Evaluate the PlantsByWebSphere Application"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#evaluate-the-complex-defaultappliction-application","text":"Navigate back to the Transformation Advisor Recommendations page. Select the DefaultApplication.ear application. As you can see from the Summary list, this is a more complex application to modernize. View the Application Technology report for DefaultApplication.ear application. As you review the Technology report, you will discover that the application makes extensive use of EJB Entity Beans. EJB Entity beans are a deprecated Java EE technology, and not supported in WebSphere Liberty, which is the preferred WebSphere edition for Cloud environments. The modernization of this application is more complex due to the need to rewrite parts of the application with Java EE technologies that are supported on WebSphere Liberty. However, WAS Base in containers is an alternative for running the application in IBM Cloud Pak for Applications with RedHat OpenShift. This option would allow you to move the application to cloud in containers (IBM Cloud Pak for Applications with Redhat OpenShift) with little or NO code changes. As the technology report reveals, the EJB Entity Beans technologies are supported in the traditional WAS Base and WAS ND editions. WAS Base in containers is a supported configuration in IB Cloud Pak for Applications. So, for this application, you might choose to move the application to WAS Base in containers, on cloud.","title":"Evaluate the Complex DefaultAppliction Application"},{"location":"ITC-19_v1.2-Evaluate_WAS_Apps_Transformation_Advisor_LABGUDE-tgf/#summary","text":"In this lab, you learned how to evaluate the existing Java application using IBM Transformation Advisor . As a part of IBM App Modernization solutions in IBM Cloud Pak for Applications , the Transformation Advisor tool provides a recommendation for the right-fit IBM WebSphere Application Server edition and offers advice, best practices and potential solutions to assess the ease of moving apps to Liberty or to WAS container, or to newer versions of WebSphere traditional. It will accelerate application migrating to cloud process, minimize errors and risks and reduce time to market. To learn more about IBM App Modernization solutions, please visit Cloud Pak for Applications . Congratulations! You have successfully completed the lab \u201cEvaluate On-Premises Java Apps with Transformation Advisor\u201d.","title":"Summary"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/","text":"ITC-20: App Modernization using WAS Base Container in IBM Cloud Pak for Applications & RedHat OpenShift Lab: App Modernization Using WAS Base Container on OCP On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to cloud quickly and cost-effectively. The IBM Cloud Pak for Applications provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. One of the tools included in the Cloud Pak for Applications is the IBM Cloud Transformation Advisor, a tool that is available at no charge to help you quickly evaluate on-premises Java EE applications and to prepare the apps for a rapid cloud deployment. IBM Cloud Transformation Advisor provides recommendations for the right-fit IBM WebSphere Application Server edition, and offers advice, best practices, and potential solutions to assess the ease of moving apps to Liberty or newer versions of WebSphere traditional on-premises or to the cloud. It accelerates application modernization to cloud while minimizing risk and reducing time to market. This lab exercise is a part of the Application Modernization lab series which focus on re-platforming WebSphere on-premise application as part of the modernization to cloud journey. We will demonstrate how to move a selected Java application from traditional WebSphere Application Server (WAS) environment to a WAS container without any code change and to deploy it to Red Hat OpenShift (OpenShift) cluster environment in IBM Cloud Pak for Applications . For more information on how to install a local version of the Transformation Advisor , please visit: https://www.ibm.com/cloud/architecture/tutorials/install-ibm-transformation-advisor-local Business Scenario As illustrated below, your company has a web app called Mod Resorts , a WebSphere application showing the weather in various locations. Your company wants to move this application from on-premises (in VMs) to the cloud (in Containers) using IBM Cloud Pak for Applications . This transition delivers improved operational efficiencies for the application by leveraging scalable cloud capabilities with IBM Cloud Pak for Applications that the enterprise currently enjoys with its cloud native applications. As a tech lead, you have already analyzed the application using IBM Cloud Transformation Advisor. Based on the analysis, you know that you can move this application from the traditional WebSphere Server ND cell environment to a containerized WAS Base server environment without any code change. Now you are planning to package the application in WAS Base traditional container and to deploy it to an OpenShift Kubernetes cluster environment in IBM Cloud Pak for Applications . By moving an application from a traditional WAS ND cell to a WAS Base container in cloud, you can take advantage of cloud to support application high availability and scalability, minimize application migration effort, reduce on-prem infrastructure and operation cost while leveraging your team\u2019s existing WebSphere admin skills. Objective When you have completed this lab, you will: understand how to move a WebSphere app from an on-premises environment to the OpenShift cluster without any changes to the app code, while utilizing exiting WebSphere administration skills. learn the process to create a WAS docker container. get familiar with the WAS Base container on OpenShift deployment process. Prerequisites The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information. What is Already Completed Four (4) Linux VMs have been provided for this lab. The OpenShift Container Platform, OCP v4.2 , included with Cloud Pak for Applications , is installed in four VMs, the master1 VM, the dns VM, the worker1 VM, with one master node and one compute node. The CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/cp4a-labs/lab2b directory of the Workstation VM. You can use this to copy / paste these commands to the Terminal window during the lab. The Workstation VM is the one you will use to access and work with OCP cluster in this lab. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd Lab Tasks During this lab, you will complete the following tasks: review the Mod Resorts app on local WAS server. build a WAS Base server container image. push the WAS Base server container image to OpenShift image repository. deploy the WAS Base server container to OpenShift cluster verify WAS server deployment. test and verify the Mod Resorts app on WAS container Execute Lab Tasks To get started, login to the environment Log in to the Workstation VM and Get Started a) If he VMs are not already started, start them by clicking the Play button b) After the VMs are started, click the Workstation VM icon to access it. The Workstation Linux Desktop is displayed. You will execute all the lab tasks on this VM. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd Review the on-prem WebSphere application In this task, you review the Modresorts application deployed to the local WAS environment. Later, you will move it to a WAS container. Start WebSphere Application Server In the Workstation VM, we have a local WebSphere Application> Server which hosts the Modresorts application. To start the WAS server: a) Open a terminal window by clicking its icon on the Workstation VM desktop tool bar. b) In the terminal window, issue the command below to start the WAS server (You can copy /paste the command from the Commands.txt file in /home/ibmdemo/cp4a-labs/lab2b /home/ibmdemo/cp4a-labs/shared/startWAS.sh when prompted, enter the \"sudo\" user password as: passw0rd Within a couple of minutes the WAS server will be started and ready for use. View the on-prem applications deployed on the WAS server. a) Access the WAS Admin Console to view the applications deployed by clicking the web browser icon desktop tool bar to open a browser window. b) From the web browser window, click WebSphere Integrated Solution Console bookmark to launch the WAS console. c) In the WAS Admin Console login page, Login using the following User ID and Password: wsadmin / passw0rd d) On the WAS Console page, click Applications -> Application Types -> WebSphere enterprise applications to view the applications deployed. In the Enterprise Applications list, you can see all applications deployed. The Mod Resorts application is in the list, it is the application you are going to move it a WAS container and to deploy it to the OpenShift cluster. View the Mod Resorts application. a) From the web browser window, click new Tab to open a new browser window with the Modresorts application URL: http://localhost:9080/resorts/ The Modresorts application home page is displayed. Stop the WAS server by issuing the following command in the Terminal window: sudo /opt/IBM/WebSphere/AppServer/profiles/AppSrv01/bin/stopServer.sh server1 when prompted enter the sudo password as **passw0rd** Then enter the WAS server credentials as: wssadmin / passw0rd Next, you will learn how to re-platform the application in WAS container without any code change and to deploy it to an OpenShift cluster in IBM Cloud Pak for Applications. Build a WAS Base Server Container Image In this task, you will build a WAS Base server Docker container image with the Mod Resorts application installed. According to Docker\u2019s best practices, you should create a new WAS base image which adds a single application and the corresponding configuration. You should avoid configuring the image manually (after it is started) vis the Admin Console or wsadmin commands unless it is for debugging purposes. Such changes will not be persisted if you spawn a new container from the Docker image. There are five key files you needed to build your WAS Base server container image: Dockerfile : The file defines how the Docker image that has your application and configuration pre-loaded is built App runtime: The ear or war file of your application app-install.props : The file defines how to install your application in the WAS server appConfig.py: The WAS admin script in Jython format configures the WAS server for your application PASSWORD: The file contains the WAS server console password use in the WAS container Review Dockerfile, app-install.props file, and appConfig.py files a) From the Desktop tool bar, click the File Manager icon to open it. b) Navigate to /home/ibmdemo/cp4a-labs/lab2b directory. c) Double click the Dockerfile to open it in Text editor for reviewing. As you can see, the Dockerfile file defines the following activities to create a WAS Base container image: Get the base WAS image from Docker Hub Add WAS admin console password to the WAS Base image Add the WAS server configuration script to the WAS base image Add the application installation script to the WAS base image Add the application runtime file to the WAS base image Run the configuration script to config the WAS server instance inside the container to configure the WAS server and to install the application d) Go back to File Manager and double click app-install.props file to review it. This is the file that defines the properties for installing the Modresorts application to the WAS server instance. e) In the File Manager window, double click appConfig.py file to review its contents. This is a standard WAS admin script with Jython format for configuring WAS to run the Mod Resorts application. This is the WAS admin script file you use to convert your application settings, include JDBC and JMS resources, from a WAS ND cell to a WAS Base container. Build the WAS Base server container image a) Go back to the Terminal window and navigate to the /home/ibmdemo/cp4a-labs/lab2b directory with command cd /home/ibmdemo/cp4a-labs/lab2b b) Execute the following command to build the WAS Base docker container image using the Dockerfile you just reviewed: docker build . -t default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/modresorts-twas:latest This creates a WAS Base docker image called default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/modresorts-twas:latest Where: - default-route-openshift-image-registry.apps.demo.ibmdte.net is the OpenShift image registry Push URL - demo is the OpenShift namespace - *modresorts-twas:latest is the image name c) After the docker container image is created, you can issue the command below to verify the docker image was created: docker images | grep twas You see the docker image is created. d) To verify the docker image and the application, create and run a local container called modresorts-twas with the command below: docker run --name modsorts-twas -p 9443:9443 -d default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/ modresorts-twas:latest Once the container is created, you can test the Moderesorts application by launching it in a web browser window with the URL: https://localhost:9443/resorts At this point, you have built the Docker container and tested the ModResorts application in the local docker environment. Next, you will push the Docker image to the OpenShift image registry and deploy the application to the RedHat OpenShift cluster in the lab environment. Push the WAS Container Image to OpenShift Image Registry After the WAS container image is built, you need to push it to an image registry. In this lab, you are using the OpenShift Image Registry to host your WAS container image. From the Terminal window, issue the command below to login to the OpenShift cluster. oc login -u admin -p passw0rd Create a new OpenShift project (namespace) called demo to host the image oc new-project demo Execute the following commands to login to the OpenShift image repository and to push your docker image docker login -u admin -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/modresorts-twas:latest Verify the pushed Docker image was successful pushed to the OpenShift image repository. a) Click the web browser icon on the desktop toolbar to go back to the browser window. b) Click OpenShift web console bookmark to open the OCP console. c) The OCP web console login page is displayed. Click the kube:admin login link d) Click to select kubeadmin username and click Log In . The OCP Web Console Home page is displayed. e) From the OCP Web Console Home page, click Builds > Images Streams from the menu f) Using the Drop-Down menu, change project (namespace) to Project: demo g) You can see the image you just pushed is listed as an available Image Streams h) Click the modresorts-twas Image Stream link to view its details. In the Image Stream Details Page Overview section, you will see the public image repository you used to push the image. Notice that the public image repository is mapped to an internal image repository which will be used to deploy the application. The internal image repository is: image-registry.openshift-image-registry.svc:5000/demo/modresorts-twas Deploy the WAS Container to OpenShift Cluster Now that the WAS container image is available in the OpenShift image registry, you will deploy the WAS container to the OpenShift cluster using the OpenShift \u201c oc \u201d CLI. Kubernetes resource files (YAML) are needed to deploy the application to OpenShift (Kubernetes Cluster). Take a moment to review the deployment resources used in the lab. View the deployment YAML files. In this lab you are going to use three YAML files to deploy the WAS container, these files are: deploy.yaml \u2013 create a WAS container in the OpenShift cluster service.yaml \u2013 create a service to expose the WAS container route.yaml \u2013 create a route to access the WAS container service a) From the File Manager window, navigate to /home/ibmdemo/cp4a-labs/lab2b/deploy directory where these three YAML files are located. b) Double-click the deploy.yaml file to open it in the text editor. The file contains the basic requirements of the WAS container deployment. The container name is set as modresorts-app-twas and it is deployed to the demo namespace. c) Double-click the service.yaml file to view its contents. The file exposes the Modresorts application ports 9080 and 9443 . The port type in the OpenShift cluster is set as ClusterIP . d) Double-click the route.yaml file to view its contents. The file defines the OpenShift route used for the WAS container as modresorts-app-twas.apps.demo.ibmdte.net which enables users to access the Modresorts application on a public address. Deploy the WAS container to OpenShift cluster using the deployment YAML files that you reviewed. a) From the Terminal window, change to the /home/ibmdemo/cp4a-labs/lab2b directory cd /home/ibmdemo/cp4a-labs/lab2b b) Issue OpenShift CLI command to deploy the WAS container to the OpenShift cluster oc apply -f deploy The command deploys the resources defined in the three YAML files in the deploy directory and create WAS container pod, service, and route in the demo namespace. Next, you will verify the deployment of the ModResorts application running in OpenShift Verify the Deployment In this task, you access the OpenShift Web Console to verify the WAS container deployment Go back to the OpenShift Web Console , click Workloads > Pods , then select demo project from the project list, if it not already selected. You will see that the WAS container is deployed and is in running status. Click the modresorts pod link to go to its overview page where you can see its memory and CPU usage, among other detail information. Now from the OpenShift Wen Console navigation panel, click Networking > Services . You will see the WAS container service . Click the service name to view its details. In the Service Details page, you can see the service ports as you defined in the service.yaml file. Next navigate to Networking > Route to view the WAS container service route. Click the Location URL to launch the Modresort application. Type the Modresorts application context root resorts in the end of the URL and press Enter . The full URL is: https://modresorts-app-twas.apps.demo.ibmdte.net/resorts The Modresorts application home page is displayed. Congratulations! You have successfully completed *App Modernization using WAS Base Container on OCP Lab! * Summary In this lab, you learned the App Modernization re-platform process to move an existing traditional WAS application to WAS Base container and to deploy it to an OpenShift cluster without any code change. IBM Cloud Pak for Applications , built on the Red Hat\u00ae OpenShift\u00ae Container Platform, provides a long-term solution to help you transition between public, private, and hybrid clouds, and to create new business applications. As a key component of IBM Cloud Pak for Applications , traditional WebSphere application Server can still run in containers, and reap the benefits of consistency and reliability that containers provide.","title":"ITC-20:"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#itc-20","text":"","title":"ITC-20:"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#app-modernization-using-was-base-container-in-ibm-cloud-pak-for-applications-redhat-openshift","text":"","title":"App Modernization using WAS Base Container in IBM Cloud Pak for Applications &amp; RedHat OpenShift"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#lab-app-modernization-using-was-base-container-on-ocp","text":"On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to cloud quickly and cost-effectively. The IBM Cloud Pak for Applications provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. One of the tools included in the Cloud Pak for Applications is the IBM Cloud Transformation Advisor, a tool that is available at no charge to help you quickly evaluate on-premises Java EE applications and to prepare the apps for a rapid cloud deployment. IBM Cloud Transformation Advisor provides recommendations for the right-fit IBM WebSphere Application Server edition, and offers advice, best practices, and potential solutions to assess the ease of moving apps to Liberty or newer versions of WebSphere traditional on-premises or to the cloud. It accelerates application modernization to cloud while minimizing risk and reducing time to market. This lab exercise is a part of the Application Modernization lab series which focus on re-platforming WebSphere on-premise application as part of the modernization to cloud journey. We will demonstrate how to move a selected Java application from traditional WebSphere Application Server (WAS) environment to a WAS container without any code change and to deploy it to Red Hat OpenShift (OpenShift) cluster environment in IBM Cloud Pak for Applications . For more information on how to install a local version of the Transformation Advisor , please visit: https://www.ibm.com/cloud/architecture/tutorials/install-ibm-transformation-advisor-local","title":"Lab: App Modernization Using WAS Base Container on OCP"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#business-scenario","text":"As illustrated below, your company has a web app called Mod Resorts , a WebSphere application showing the weather in various locations. Your company wants to move this application from on-premises (in VMs) to the cloud (in Containers) using IBM Cloud Pak for Applications . This transition delivers improved operational efficiencies for the application by leveraging scalable cloud capabilities with IBM Cloud Pak for Applications that the enterprise currently enjoys with its cloud native applications. As a tech lead, you have already analyzed the application using IBM Cloud Transformation Advisor. Based on the analysis, you know that you can move this application from the traditional WebSphere Server ND cell environment to a containerized WAS Base server environment without any code change. Now you are planning to package the application in WAS Base traditional container and to deploy it to an OpenShift Kubernetes cluster environment in IBM Cloud Pak for Applications . By moving an application from a traditional WAS ND cell to a WAS Base container in cloud, you can take advantage of cloud to support application high availability and scalability, minimize application migration effort, reduce on-prem infrastructure and operation cost while leveraging your team\u2019s existing WebSphere admin skills.","title":"Business Scenario"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#objective","text":"When you have completed this lab, you will: understand how to move a WebSphere app from an on-premises environment to the OpenShift cluster without any changes to the app code, while utilizing exiting WebSphere administration skills. learn the process to create a WAS docker container. get familiar with the WAS Base container on OpenShift deployment process.","title":"Objective"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#prerequisites","text":"The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information.","title":"Prerequisites"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#what-is-already-completed","text":"Four (4) Linux VMs have been provided for this lab. The OpenShift Container Platform, OCP v4.2 , included with Cloud Pak for Applications , is installed in four VMs, the master1 VM, the dns VM, the worker1 VM, with one master node and one compute node. The CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/cp4a-labs/lab2b directory of the Workstation VM. You can use this to copy / paste these commands to the Terminal window during the lab. The Workstation VM is the one you will use to access and work with OCP cluster in this lab. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd","title":"What is Already Completed"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#lab-tasks","text":"During this lab, you will complete the following tasks: review the Mod Resorts app on local WAS server. build a WAS Base server container image. push the WAS Base server container image to OpenShift image repository. deploy the WAS Base server container to OpenShift cluster verify WAS server deployment. test and verify the Mod Resorts app on WAS container","title":"Lab Tasks"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#execute-lab-tasks","text":"","title":"Execute Lab Tasks"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#to-get-started-login-to-the-environment","text":"Log in to the Workstation VM and Get Started a) If he VMs are not already started, start them by clicking the Play button b) After the VMs are started, click the Workstation VM icon to access it. The Workstation Linux Desktop is displayed. You will execute all the lab tasks on this VM. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd","title":"To get started, login to the environment"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#review-the-on-prem-websphere-application","text":"In this task, you review the Modresorts application deployed to the local WAS environment. Later, you will move it to a WAS container. Start WebSphere Application Server In the Workstation VM, we have a local WebSphere Application> Server which hosts the Modresorts application. To start the WAS server: a) Open a terminal window by clicking its icon on the Workstation VM desktop tool bar. b) In the terminal window, issue the command below to start the WAS server (You can copy /paste the command from the Commands.txt file in /home/ibmdemo/cp4a-labs/lab2b /home/ibmdemo/cp4a-labs/shared/startWAS.sh when prompted, enter the \"sudo\" user password as: passw0rd Within a couple of minutes the WAS server will be started and ready for use. View the on-prem applications deployed on the WAS server. a) Access the WAS Admin Console to view the applications deployed by clicking the web browser icon desktop tool bar to open a browser window. b) From the web browser window, click WebSphere Integrated Solution Console bookmark to launch the WAS console. c) In the WAS Admin Console login page, Login using the following User ID and Password: wsadmin / passw0rd d) On the WAS Console page, click Applications -> Application Types -> WebSphere enterprise applications to view the applications deployed. In the Enterprise Applications list, you can see all applications deployed. The Mod Resorts application is in the list, it is the application you are going to move it a WAS container and to deploy it to the OpenShift cluster. View the Mod Resorts application. a) From the web browser window, click new Tab to open a new browser window with the Modresorts application URL: http://localhost:9080/resorts/ The Modresorts application home page is displayed. Stop the WAS server by issuing the following command in the Terminal window: sudo /opt/IBM/WebSphere/AppServer/profiles/AppSrv01/bin/stopServer.sh server1 when prompted enter the sudo password as **passw0rd** Then enter the WAS server credentials as: wssadmin / passw0rd Next, you will learn how to re-platform the application in WAS container without any code change and to deploy it to an OpenShift cluster in IBM Cloud Pak for Applications.","title":"Review the on-prem WebSphere application"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#build-a-was-base-server-container-image","text":"In this task, you will build a WAS Base server Docker container image with the Mod Resorts application installed. According to Docker\u2019s best practices, you should create a new WAS base image which adds a single application and the corresponding configuration. You should avoid configuring the image manually (after it is started) vis the Admin Console or wsadmin commands unless it is for debugging purposes. Such changes will not be persisted if you spawn a new container from the Docker image. There are five key files you needed to build your WAS Base server container image: Dockerfile : The file defines how the Docker image that has your application and configuration pre-loaded is built App runtime: The ear or war file of your application app-install.props : The file defines how to install your application in the WAS server appConfig.py: The WAS admin script in Jython format configures the WAS server for your application PASSWORD: The file contains the WAS server console password use in the WAS container Review Dockerfile, app-install.props file, and appConfig.py files a) From the Desktop tool bar, click the File Manager icon to open it. b) Navigate to /home/ibmdemo/cp4a-labs/lab2b directory. c) Double click the Dockerfile to open it in Text editor for reviewing. As you can see, the Dockerfile file defines the following activities to create a WAS Base container image: Get the base WAS image from Docker Hub Add WAS admin console password to the WAS Base image Add the WAS server configuration script to the WAS base image Add the application installation script to the WAS base image Add the application runtime file to the WAS base image Run the configuration script to config the WAS server instance inside the container to configure the WAS server and to install the application d) Go back to File Manager and double click app-install.props file to review it. This is the file that defines the properties for installing the Modresorts application to the WAS server instance. e) In the File Manager window, double click appConfig.py file to review its contents. This is a standard WAS admin script with Jython format for configuring WAS to run the Mod Resorts application. This is the WAS admin script file you use to convert your application settings, include JDBC and JMS resources, from a WAS ND cell to a WAS Base container. Build the WAS Base server container image a) Go back to the Terminal window and navigate to the /home/ibmdemo/cp4a-labs/lab2b directory with command cd /home/ibmdemo/cp4a-labs/lab2b b) Execute the following command to build the WAS Base docker container image using the Dockerfile you just reviewed: docker build . -t default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/modresorts-twas:latest This creates a WAS Base docker image called default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/modresorts-twas:latest Where: - default-route-openshift-image-registry.apps.demo.ibmdte.net is the OpenShift image registry Push URL - demo is the OpenShift namespace - *modresorts-twas:latest is the image name c) After the docker container image is created, you can issue the command below to verify the docker image was created: docker images | grep twas You see the docker image is created. d) To verify the docker image and the application, create and run a local container called modresorts-twas with the command below: docker run --name modsorts-twas -p 9443:9443 -d default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/ modresorts-twas:latest Once the container is created, you can test the Moderesorts application by launching it in a web browser window with the URL: https://localhost:9443/resorts At this point, you have built the Docker container and tested the ModResorts application in the local docker environment. Next, you will push the Docker image to the OpenShift image registry and deploy the application to the RedHat OpenShift cluster in the lab environment.","title":"Build a WAS Base Server Container Image"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#push-the-was-container-image-to-openshift-image-registry","text":"After the WAS container image is built, you need to push it to an image registry. In this lab, you are using the OpenShift Image Registry to host your WAS container image. From the Terminal window, issue the command below to login to the OpenShift cluster. oc login -u admin -p passw0rd Create a new OpenShift project (namespace) called demo to host the image oc new-project demo Execute the following commands to login to the OpenShift image repository and to push your docker image docker login -u admin -p $(oc whoami -t) default-route-openshift-image-registry.apps.demo.ibmdte.net docker push default-route-openshift-image-registry.apps.demo.ibmdte.net/demo/modresorts-twas:latest Verify the pushed Docker image was successful pushed to the OpenShift image repository. a) Click the web browser icon on the desktop toolbar to go back to the browser window. b) Click OpenShift web console bookmark to open the OCP console. c) The OCP web console login page is displayed. Click the kube:admin login link d) Click to select kubeadmin username and click Log In . The OCP Web Console Home page is displayed. e) From the OCP Web Console Home page, click Builds > Images Streams from the menu f) Using the Drop-Down menu, change project (namespace) to Project: demo g) You can see the image you just pushed is listed as an available Image Streams h) Click the modresorts-twas Image Stream link to view its details. In the Image Stream Details Page Overview section, you will see the public image repository you used to push the image. Notice that the public image repository is mapped to an internal image repository which will be used to deploy the application. The internal image repository is: image-registry.openshift-image-registry.svc:5000/demo/modresorts-twas","title":"Push the WAS Container Image to OpenShift Image Registry"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#deploy-the-was-container-to-openshift-cluster","text":"Now that the WAS container image is available in the OpenShift image registry, you will deploy the WAS container to the OpenShift cluster using the OpenShift \u201c oc \u201d CLI. Kubernetes resource files (YAML) are needed to deploy the application to OpenShift (Kubernetes Cluster). Take a moment to review the deployment resources used in the lab. View the deployment YAML files. In this lab you are going to use three YAML files to deploy the WAS container, these files are: deploy.yaml \u2013 create a WAS container in the OpenShift cluster service.yaml \u2013 create a service to expose the WAS container route.yaml \u2013 create a route to access the WAS container service a) From the File Manager window, navigate to /home/ibmdemo/cp4a-labs/lab2b/deploy directory where these three YAML files are located. b) Double-click the deploy.yaml file to open it in the text editor. The file contains the basic requirements of the WAS container deployment. The container name is set as modresorts-app-twas and it is deployed to the demo namespace. c) Double-click the service.yaml file to view its contents. The file exposes the Modresorts application ports 9080 and 9443 . The port type in the OpenShift cluster is set as ClusterIP . d) Double-click the route.yaml file to view its contents. The file defines the OpenShift route used for the WAS container as modresorts-app-twas.apps.demo.ibmdte.net which enables users to access the Modresorts application on a public address. Deploy the WAS container to OpenShift cluster using the deployment YAML files that you reviewed. a) From the Terminal window, change to the /home/ibmdemo/cp4a-labs/lab2b directory cd /home/ibmdemo/cp4a-labs/lab2b b) Issue OpenShift CLI command to deploy the WAS container to the OpenShift cluster oc apply -f deploy The command deploys the resources defined in the three YAML files in the deploy directory and create WAS container pod, service, and route in the demo namespace. Next, you will verify the deployment of the ModResorts application running in OpenShift","title":"Deploy the WAS Container to OpenShift Cluster"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#verify-the-deployment","text":"In this task, you access the OpenShift Web Console to verify the WAS container deployment Go back to the OpenShift Web Console , click Workloads > Pods , then select demo project from the project list, if it not already selected. You will see that the WAS container is deployed and is in running status. Click the modresorts pod link to go to its overview page where you can see its memory and CPU usage, among other detail information. Now from the OpenShift Wen Console navigation panel, click Networking > Services . You will see the WAS container service . Click the service name to view its details. In the Service Details page, you can see the service ports as you defined in the service.yaml file. Next navigate to Networking > Route to view the WAS container service route. Click the Location URL to launch the Modresort application. Type the Modresorts application context root resorts in the end of the URL and press Enter . The full URL is: https://modresorts-app-twas.apps.demo.ibmdte.net/resorts The Modresorts application home page is displayed. Congratulations! You have successfully completed *App Modernization using WAS Base Container on OCP Lab! *","title":"Verify the Deployment"},{"location":"ITC-20_v2.2-App_Modernization_WAS_Base_Container_OSCP_LABGUDE-tgf/#summary","text":"In this lab, you learned the App Modernization re-platform process to move an existing traditional WAS application to WAS Base container and to deploy it to an OpenShift cluster without any code change. IBM Cloud Pak for Applications , built on the Red Hat\u00ae OpenShift\u00ae Container Platform, provides a long-term solution to help you transition between public, private, and hybrid clouds, and to create new business applications. As a key component of IBM Cloud Pak for Applications , traditional WebSphere application Server can still run in containers, and reap the benefits of consistency and reliability that containers provide.","title":"Summary"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/","text":"ITC-21 Create and Deploy a Cloud Native App to IBM Cloud Pak for Applications (CP4Apps) Lab: Create and Deploy an Insurance Quote App to OpenShift with IBM Cloud Pak for Applications On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to cloud and creating cloud native business applications quickly and cost-effectively. The IBM Cloud Pak for Applications (CP4Apps) provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. One of the features included in the Pak is \u2018Accelerators for Teams\u2019, a modern microservices-based framework that enables developers, architects and operation teams to work together, faster on end-to-end solutions for the team to build, deploy, and manage the lifecycle of Kubernetes-based applications on OpenShift Container Platform. Accelerators for Teams provides Application Stacks, Pipelines for modern DevOps and Developer Tools. These features are derived from Kabanero, an open source project that integrates many open source projects and is the upstream project for Cloud Pak for Applications. You can get starting using only open source with Kabanero and try out capabilities in Cloud Pak for Applications. Learn more at https://kabanero.io For more information about IBM Cloud Pak for Applications, please visit Cloud Pak for Applications . In this lab you will first learn how to develop a cloud-native microservice application using pre-built Application Stacks, customized for your enterprise. You will use a local iterative dev experience with a CLI to rapidly develop a microservice with a pre-built Application Stack and deploy to Red Hat OpenShift. The Appsody CLI will be used. Then you will learn how to use different stacks to develop microservices applications and deploy the applications to Red Hat OpenShift using different techniques including Kubernetes-native pipelines based on an open source project called Tekton from the Continuous Delivery Foundation. This provides the preferred and governed way applications should be verified, built and deployed to OpenShift clusters consistently. As shown in the diagram below, Application stacks (built using Appsody) simplify the process of building and deploying applications to OpenShift for both modernized and new cloud-native apps. Business Scenario The lab covers the following business scenario: Your company is developing a cloud native microservice application to provide new insurance quote services to your customers. The application consists of a frontend web UI service application and a backend REST application. Your team architect has designed to use a Node.js Express stack for the web application and to use the Spring Boot stack for the backend application. He has made these available to developers in a local Hub called a \u201c Collection Hub \u201d which brings together the Customized Application Stacks and the Pipelines that control build and deployment consistency on the enterprise\u2019s OpenShift cluster. As a developer, you will configure your developer tools to this Local Hub to create the cloud native applications using the customized Application Stacks (nodejs-express and spring boot) that meets your enterprises standards and multi-disciplinary team\u2019s decisions. You will then deploy them to the OpenShift Container Platform. Objective The objectives of this lab are to: Learn how to use the Appsody CLI to create cloud native microservice applications with Application Stacks. Learn how to deploy microservice applications to OpenShift using pre-built yet customizable Pipelines that are Kubernetes-native, based on an open source project called Tekton. Prerequisites The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands Have internet access Have a GitHub account. If you do not have it yet, you can sign up for a free account at: https://github.com The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information. What is Already Completed Four (4) Linux VMs have been provided for this lab. The OpenShift Container Platform, OCP v4.2 , included with Cloud Pak for Applications , is installed in four VMs, the master1 VM, the dns VM, the worker1 VM, with one master node and one compute nodes. The IBM Cloud Pak for Applications has been deployed to the OCP cluster . For information on how to install IBM Cloud Pak for Applications on OpenShift, please visit: https://www.ibm.com/support/knowledgecenter/SSCSJL_4.x/welcome.html Appsody client (v0.5.4) is installed on the Workstation VM . The CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/cp4a-labs/lab8 directory of the Workstation VM. You can use theses commands to copy and paste commands to the Terminal window during this lab. The Workstation VM is the one you will use to access and work with OCP cluster in this lab. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab Introduction to Application Stacks built using Appsody: How does it work? Application Stacks built using Appsody, an open source project, use the well-known Dockerfile syntax to specify which language runtimes, frameworks, libraries, and tools are included. Project templates build upon these stacks, providing developers with a templatized starter application to bring into their IDE of choice and begin development. Developers run, debug, or test their application using a CLI from Appsody. It starts a container with the stack image, makes the development workspace available to the running container, and starts the Appsody controller. The controller is configurable through environment variables in the stack and manages the application within the running container. For example, the controller can watch for changes in the application /src directory. When these changes are saved, the controller restarts the application running in a docker container. Application Stacks , built using Appsody, are configurable technology stacks with popular runtimes and frameworks. Two examples are: Java with Eclipse MicroProfile using Open Liberty and Node.js with Express . These stacks provide the foundation for building applications that can be deployed and managed effectively on OpenShift. Stacks allow for rapid development, whilst giving the stack provider the ability to control how the applications are composed. Building a new or modifying an application stack is similar to creating a custom Cloud Foundry build pack. We are building an enterprise curated application container that provide tools and a Docker (CRI-O) container that will be used to build a specific application kind (e.g., java-microprofile, spring-boot2, node.js-express, etc.). Stack Hubs The Stack Hub is the central point of control for Application Stacks, built using Appsody, where you can find available stacks, create new stacks, or modify existing ones. You can use the Hub content in the public repo or clone it to provide a private Hub that's based on your requirements. By making changes to the Stacks in the Hub, you can deploy updates to any application that's been built on them, simply by restarting the application. Collections are simply an Application Stack and the Pipelines required to deploy them. A Collection Hub is a Stack Hub plus additional folder structures that store pipelines for build and deployment. When CP4Apps is installed, it is the solution architect\u2019s role to define and customize the Application Stacks made available to developers. This is a multi-disciplinary process where Developers, Cloud Operators and Solution Architects come together and agree on choices to codify in the Application Stack and Pipelines. These customizations are done in the Local Hub in their private Git Hub Enterprise or Git Lab deployment. Stack images are copied into a Local Registry where they are used to ensure image builds occur in consistent and governed manner, when deploying through pipelines onto remote OpenShift clusters. For lab simplicity, we\u2019ll start with an application stack that is provided by the Appsody open source community . Lab Tasks During this lab, you will complete the following tasks: create native applications using Application Stacks based on Appsody. modify the applications with insurance quote business logic Use Appsody CLI to build, deploy, and validate the front-end application locally in containers deploy the quote frontend application to OpenShift using a CI/CD pipeline based on Tekton. In this lab you'll be doing everything via the command line so that you can understand the steps in detail. If you are using an IDE like VisualStudio Code , Eclipse , or the hosted Eclipse Che IDE (CodeReady Workspaces on OpenShift), the Eclipse Codewind extension provides the same function to create, run, debug, and test these applications without using the command line or leaving the IDE. Execute Lab Tasks Log in to the Workstation VM and Get Started a) If he VMs are not already started, start them by clicking the Play button b) After the VMs are started, click the Workstation VM icon to access it. The Workstation Linux Desktop is displayed. You will execute all the lab tasks on this VM. If requested to login to the OS, use credentials: ibmdemo / passw0rd Add the Collection Hub repo to Appsody Appsody uses sharable technology stacks for rapid development of quality microservice-based applications. Each Application Stack is pre-configured with popular open source technologies. Application Stacks include a base container image (UBI from Red Hat for all Stacks in CP4Apps) and project templates which act as a starting point for your application development. In this task, you will add the local Collection Hub built by the enterprises Solution Architect to Appsody. From the Workstation desktop, open a Terminal window by clicking its icon on the Desktop Tool Bar. In the Terminal window, issue the following command to add the Collection Hub to the local Appsody repo list: appsody repo add kabanero https://github.com/kabanero-io/collections/releases/download/0.3.5/kabanero-index.yaml Where kabanero is the collection repo name The kabanero repo is now added to Appsody. To verify it, issue command: appsody repo list The command returns the list of repos in the collection To view the available stacks included in the kabanero repo, run the following command: appsody list kabanero This will give you a list of the available stacks. Notice that the kabanero repo contains Application Stacks for specific versions of Nodejs, Springboot, Microprofile. Set the kabanero repo as the default Appsody repo with command: appsody repo set-default kabanero The default Appsody repo is now set. You will use the nodejs-express stack to develop the insurance quote front-end application. Create the frontend application and run it locally The frontend application is written in Node.js Express. First let's initialize a new project that uses the Node.js Express stack. In the terminal window, create a directory named quote-frontend . Create a new node.js application using appsody init command shown below: mkdir /home/ibmdemo/quote-frontend cd quote-frontend appsody init kabanero/nodejs-express The new node.js application is now successfully initialized, as illustrated below. After the appsody init command completes, launch Visual Studio Code (VS Code) editor with the command below to view the content of the directory. code . The Application Stack has a simplified project structure on disk. Note that this project doesn't contain a Dockerfile, Node, Express, or many of the other files that you might expect in a cloud-native Express project. With Application Stacks built using Appsody these files - along with many best practices and opinionated decisions are provided by the stack, resulting in automatically following best practices and a much simpler view for the developer. Here is an example of what\u2019s included in an Application Stack: In this lab, you are going to replace the starter code created in the init process with the insurance quote frontend application code. a) First, edit the package.json file and add the following to the dependencies section: \"dependencies\": { \"body-parser\": \"^1.19.0\", \"config\": \"^3.2.0\", \"express-validator\": \"^6.2.0\", \"pug\": \"^2.0.0\", \"request\": \"^2.88.0\" Refer to the screen shot below to see where to place the text in the package.json file. Click File > Save All to change the change Then, Exit VS Code. Now, from the Terminal window, copy the insurance quote frontend application code files from the /home/ibmdemo/cp4a-labs/lab8/exercise-frontend directory to your Appsody project: cp -R /home/ibmdemo/cp4a-labs/lab8/exercise-frontend/* . The resulting directory structure of your project is illustrated below: Stop Transformation Advisor Docker container that is running on port 3000, which conflicts with Appsody. This is just to work around our lab environment configuration. /home/ibmdemo/ta-local/transformationAdvisor/scripts/stopTransformationAdvisor.sh Start the application locally, using the following command: appsody run You can see the output of the application by launching a web browser window and navigating to http://localhost:3000 The Insurance Quote form is displayed, as illustrated below You can fill in the form and hit the button to submit it and a response will appear like this: . Note: In this case, the frontend application is not sending a request to the backend application. Instead it is configured to use a mock endpoint for testing purposes in development mode. This works as follows. quote.js uses the config module to get the value for the backend URL. When the application runs in development mode, the config module uses config/development.json to find the value for the backend URL. This file sets the URL to the mock endpoint in the config/development.json file. { \"backendUrl\": \"http://localhost:3000/quote/test\" } When the application runs in production mode (which we will see later), the config module uses config/custom-environment-variables.json file to find the value for the backend URL. This file sets the URL from the BACKEND_URL environment variable. { \"backendUrl\": \"BACKEND_URL\" } Go back to the Terminal window and enter Ctrl+C to stop the application. Exit the VS Code UI, if it is still open Congratulations! You have successfully used capabilities built into IBM Cloud Pak for Applications to create, build, and deploy a cloud native application in a local containerized environment. * In the next section, you will leverage the out-of-the-box Pipelines included with IBM Cloud Pak for Applications to build, scan, and deploy the cloud- native application to OpenShift. More information on Appsody, an Open-Source project under the Kabanero Open-Source project: Appsody documentation on CLI commands: https://appsody.dev/docs/using-appsody/cli-commands/ Appsody documentation for Testing stacks: https://appsody.dev/docs/stacks/test Let\u2019s recap what we have done so far First, you created a new local collection hub based on a public collection hub You listed the available application stacks in the local collection hub You made your \u201ckabanero\u201d stack the default stack in the local collection hub You initialized the \u201cnodejs-express\u201d application stack in the dev.local repo You used Appsody CLI to \u201c run \u201d the \u201cfront-end\u201d nodejs-express application locally in containers, using Appsody CLI. This also built the application using the out-of-the-box-resources in the stack. Finally, you used the web browser to test the \u201cfront-end\u201d nodejs-express application stack from the kabanero collection. Deploy the quote-frontend application to OpenShift with the CI/CD Pipeline In this section, you will deploy the frontend application to OpenShift. You will use a Pipeline based on Tekton to deploy the application in a controlled and secure manner onto the OpenShift Container Platform. Note: Tekton is the Open-Source upstream project for RedHat OpenShift Pipelines. Enterprise solution architects generally will work with Cloud Operators who manage OpenShift production clusters and developers to create approved pipelines with steps that meet their enterprise\u2019s needs. Navigate to your quote-frontend directory. We first create the deployment yaml as follows: cd /home/ibmdemo/quote-frontend appsody deploy --generate-only This creates a file named app-deploy.yaml in your project folder. It will take a couple of minutes for the task to complete. On-line documentation for Appsody deploy: https://appsody.dev/docs/cli-commands/#appsody-deploy Now we need to tell the frontend application how to talk to the backend application. The frontend will look at an environment variable BACKEND_URL to find the address of the backend, so we will set this in the application Custom Resource (CR). a) Exit VS Code UI, if it is still running. b) Then, relaunch VS Code from the current directory of your quote-frontend app: code . c) From the VS Code edit, open the app-deploy.yaml file and add the following section as shown below: env: - name: BACKEND\\_URL value: <http://quote-backend:8080/quote> Notes: a. Your line numbers may be different if the template code has changed since the screenshot. b. This is yaml file, so indentation matters. And No TABS! d) Save the change by clicking File > Save All e) Exit VS Code. Create a Git repo for the application. You will push the application to your GitHub repo. This step requires that you have a public GitHub account. If you do not have a GitHub account, create a free account at: https://github.com a) From the web browser window, click the GitHub bookmark to launch it. b) Log in to your existing GitHub account. If you do not have one, you can sign up to get a free account. c) From your GitHub page, click Add>New repository to create a new repo. d) Enter the Repository name as quote-frontend, select the Public option and click Create repository. e) Your repository is created. Write down the GitHub repo URL link of the repository, you are going to use it later. f) From the Terminal window, push your application code to the Git repo, using the following commands: cd /home/ibmdemo/quote-frontend git init git add . git commit -m \"first commit\" git remote add origin https://github.com/<your Github User Name>/quote-frontend.git Note: where <your Github User Name> is your GitHub user name. git push -u origin master Note: When prompted, enter your GitHub username and password Refresh your GitHub Repo and verify the contents have been published to the quote-frontend repo Add the Pipeline resources to OpenShift Login to the OpenShift Cluster oc login -u admin -p passw0rd Set up a project namespace. OpenShift applications are deployed within a project. The first step in the deployment process is to create a new or select an existing project. In this lab, we are going to use an existing project called kabanero . From the Terminal window, issue the following commands: oc project kabanero Note: You are now working in the \u201c kabanero \u201d project in OpenShift. Create a new Tekton pipeline CR. (Kubernetes Custom Resource) Note: In general, developers don\u2019t create the pipelines. They should be setup for them with all the tasks their solution architect and cloud operator teams require. In this lab, you will act as the operator to set the pipeline CR and get familiar with the process. a) Copy the sample pipeline CR file pipeline-resources.yaml from /home/ibmdemo/cp4a-labs/lab8/exercise-frontend/ directory to /home/ibmdemo/quote-frontend directory, using the following commands: cd /home/ibmdemo/quote-frontend cp /home/ibmdemo/cp4a-labs/lab8/exercise-frontend/pipeline-resources.yaml . b) Return to the VS Code editor, or launch it, if it is not running. code . c) Click pipeline-resources.yaml file to open it, change the GitHub user to your Github username d) Click File > Save All to save the change. e) Exit VS Code f) Deploy the pipeline resource to the OCP cluster kabanero namespace, using the following commands: cd /home/ibmdemo/quote-frontend oc apply -f pipeline-resources.yaml -n kabanero Deploy the quote-frontend application to OpenShft using the Tekton Piepline Access the Tekton Dashboard to setup the PipelineRun In order to provide control and consistency at scale, all build and deploy steps must go through the pipeline. So, in an enterprise setting, webhooks would be setup by the operations or solution architect that monitor the developers Git repos and trigger the PipelineRun during commit/push actions. Due to network connectivity limitations with the lab infrastructure, we are asking you to create a manual PipelineRun to simulate the webhook completing that step, enabling the rest of the lab flow to continue. a) Go back to the web browser window and launch the Tekton Dashboard by clicking its bookmark. b) Click Log in with OpenShift to continue c) In the login page, click kube:admin field, select the kubeadmin user name and click Log in . Create the PipelineRun and Run the Pipeline to deploy the application. a) From the Tekton Dashboard page, you can click the Pipelines link to view all available pipelines, or you can click PipelineResources to view the two resourced you just created. b) We are going to use the PipelineRun button to deploy the quote-frontend application, so click Create PipelineRun in the PipelineRuns page. c) In the PipelineRun page, set the parameters as shown in table below. Then click Create . Note: Refer to the screenshot below the table, if additional clarity is needed for input parameters. Namespace kabanero Pipeline nodejs-express-build-push-deploy-pipeline git quote-frontend-git image quote-frontend-image Event-header demo Service Account kabanero-operator d) The pipelinerun is created, click the its link to view the run status: e) The pipeline is running. It has several tasks, including build-push , image scan and deploy . Each task contains multiple stages. When the stage is running, you can click its link and view its logs , status and details. When the PipelineRun completes, your application is deployed to OpenShift cluster. The pipeline will take several minutes to complete all of the tasks. It downloads and installs a lot of pf modules during the build phase. It took about 15 minutes in my testing! Once the Pipeline completes, the final step in the lab is to validate the deployment to OpenShift. To verify the deployment to OpenShift a) To view your deployment in the OCP cluster, go back to the web browser window and click the OpenShift Web Console bookmark. b) If prompted to login, click the htpassword link and login using admin / passw0rd c) From the OpenShift Web Console, navigate to Workloads>Deployments and go to the kabanero project d) Click the quote-frontend application link to view its deployment details e) Next, select Networking>Routes section and click the quote-frontend application URL to lunch it. The Quote application Web page is displayed, as illustrated below: Summary In this lab, you have learned how to create a cloud native microservice application using customized Application Stacks provided by Cloud Pak for Applications. These stacks are built using an open source project called Appsody. You can learned how to use the CLI provided by Appsody to create projects, do iterative local development and deploy applications to OpenShift Pipelines provided with Cloud Pak for Applications. IBM Cloud Pak for Application s makes it faster and easier for developers to create and deploy cloud native applications on OpenShift using Application Stacks that also provide enterprise governance for cloud native development. Congratulations! You have successfully completed the lab \u201cCreate and Deploy a cloud native application to IBM Cloud Pak for Applications\u201d.","title":"ITC-21"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#itc-21","text":"","title":"ITC-21"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#create-and-deploy-a-cloud-native-app-to-ibm-cloud-pak-for-applications-cp4apps","text":"","title":"Create and Deploy a Cloud Native App to IBM Cloud Pak for Applications (CP4Apps)"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#lab-create-and-deploy-an-insurance-quote-app-to-openshift-with-ibm-cloud-pak-for-applications","text":"On the journey to cloud, enterprise customers are facing challenges moving their existing on-premises applications to cloud and creating cloud native business applications quickly and cost-effectively. The IBM Cloud Pak for Applications (CP4Apps) provides a complete and consistent experience and solution to modernize enterprise applications for cloud-native deployments. Customers can easily modernize their existing applications with IBM\u2019s integrated tools and develop new cloud-native applications faster for deployment on any cloud. One of the features included in the Pak is \u2018Accelerators for Teams\u2019, a modern microservices-based framework that enables developers, architects and operation teams to work together, faster on end-to-end solutions for the team to build, deploy, and manage the lifecycle of Kubernetes-based applications on OpenShift Container Platform. Accelerators for Teams provides Application Stacks, Pipelines for modern DevOps and Developer Tools. These features are derived from Kabanero, an open source project that integrates many open source projects and is the upstream project for Cloud Pak for Applications. You can get starting using only open source with Kabanero and try out capabilities in Cloud Pak for Applications. Learn more at https://kabanero.io For more information about IBM Cloud Pak for Applications, please visit Cloud Pak for Applications . In this lab you will first learn how to develop a cloud-native microservice application using pre-built Application Stacks, customized for your enterprise. You will use a local iterative dev experience with a CLI to rapidly develop a microservice with a pre-built Application Stack and deploy to Red Hat OpenShift. The Appsody CLI will be used. Then you will learn how to use different stacks to develop microservices applications and deploy the applications to Red Hat OpenShift using different techniques including Kubernetes-native pipelines based on an open source project called Tekton from the Continuous Delivery Foundation. This provides the preferred and governed way applications should be verified, built and deployed to OpenShift clusters consistently. As shown in the diagram below, Application stacks (built using Appsody) simplify the process of building and deploying applications to OpenShift for both modernized and new cloud-native apps.","title":"Lab: Create and Deploy an Insurance Quote App to OpenShift with IBM Cloud Pak for Applications"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#business-scenario","text":"The lab covers the following business scenario: Your company is developing a cloud native microservice application to provide new insurance quote services to your customers. The application consists of a frontend web UI service application and a backend REST application. Your team architect has designed to use a Node.js Express stack for the web application and to use the Spring Boot stack for the backend application. He has made these available to developers in a local Hub called a \u201c Collection Hub \u201d which brings together the Customized Application Stacks and the Pipelines that control build and deployment consistency on the enterprise\u2019s OpenShift cluster. As a developer, you will configure your developer tools to this Local Hub to create the cloud native applications using the customized Application Stacks (nodejs-express and spring boot) that meets your enterprises standards and multi-disciplinary team\u2019s decisions. You will then deploy them to the OpenShift Container Platform.","title":"Business Scenario"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#objective","text":"The objectives of this lab are to: Learn how to use the Appsody CLI to create cloud native microservice applications with Application Stacks. Learn how to deploy microservice applications to OpenShift using pre-built yet customizable Pipelines that are Kubernetes-native, based on an open source project called Tekton.","title":"Objective"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#prerequisites","text":"The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands Have internet access Have a GitHub account. If you do not have it yet, you can sign up for a free account at: https://github.com The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information.","title":"Prerequisites"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#what-is-already-completed","text":"Four (4) Linux VMs have been provided for this lab. The OpenShift Container Platform, OCP v4.2 , included with Cloud Pak for Applications , is installed in four VMs, the master1 VM, the dns VM, the worker1 VM, with one master node and one compute nodes. The IBM Cloud Pak for Applications has been deployed to the OCP cluster . For information on how to install IBM Cloud Pak for Applications on OpenShift, please visit: https://www.ibm.com/support/knowledgecenter/SSCSJL_4.x/welcome.html Appsody client (v0.5.4) is installed on the Workstation VM . The CLI commands used in this lab are listed in the Commands.txt file located at the /home/ibmdemo/cp4a-labs/lab8 directory of the Workstation VM. You can use theses commands to copy and paste commands to the Terminal window during this lab. The Workstation VM is the one you will use to access and work with OCP cluster in this lab. The login credentials for the Workstation VM are: User ID: ibmdemo Password: passw0rd Note: Use the Password above in the Workstation VM Terminal for sudo in the Lab","title":"What is Already Completed"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#introduction-to-application-stacks-built-using-appsody-how-does-it-work","text":"Application Stacks built using Appsody, an open source project, use the well-known Dockerfile syntax to specify which language runtimes, frameworks, libraries, and tools are included. Project templates build upon these stacks, providing developers with a templatized starter application to bring into their IDE of choice and begin development. Developers run, debug, or test their application using a CLI from Appsody. It starts a container with the stack image, makes the development workspace available to the running container, and starts the Appsody controller. The controller is configurable through environment variables in the stack and manages the application within the running container. For example, the controller can watch for changes in the application /src directory. When these changes are saved, the controller restarts the application running in a docker container. Application Stacks , built using Appsody, are configurable technology stacks with popular runtimes and frameworks. Two examples are: Java with Eclipse MicroProfile using Open Liberty and Node.js with Express . These stacks provide the foundation for building applications that can be deployed and managed effectively on OpenShift. Stacks allow for rapid development, whilst giving the stack provider the ability to control how the applications are composed. Building a new or modifying an application stack is similar to creating a custom Cloud Foundry build pack. We are building an enterprise curated application container that provide tools and a Docker (CRI-O) container that will be used to build a specific application kind (e.g., java-microprofile, spring-boot2, node.js-express, etc.).","title":"Introduction to Application Stacks built using Appsody: How does it work?"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#stack-hubs","text":"The Stack Hub is the central point of control for Application Stacks, built using Appsody, where you can find available stacks, create new stacks, or modify existing ones. You can use the Hub content in the public repo or clone it to provide a private Hub that's based on your requirements. By making changes to the Stacks in the Hub, you can deploy updates to any application that's been built on them, simply by restarting the application. Collections are simply an Application Stack and the Pipelines required to deploy them. A Collection Hub is a Stack Hub plus additional folder structures that store pipelines for build and deployment. When CP4Apps is installed, it is the solution architect\u2019s role to define and customize the Application Stacks made available to developers. This is a multi-disciplinary process where Developers, Cloud Operators and Solution Architects come together and agree on choices to codify in the Application Stack and Pipelines. These customizations are done in the Local Hub in their private Git Hub Enterprise or Git Lab deployment. Stack images are copied into a Local Registry where they are used to ensure image builds occur in consistent and governed manner, when deploying through pipelines onto remote OpenShift clusters. For lab simplicity, we\u2019ll start with an application stack that is provided by the Appsody open source community .","title":"Stack Hubs"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#lab-tasks","text":"During this lab, you will complete the following tasks: create native applications using Application Stacks based on Appsody. modify the applications with insurance quote business logic Use Appsody CLI to build, deploy, and validate the front-end application locally in containers deploy the quote frontend application to OpenShift using a CI/CD pipeline based on Tekton. In this lab you'll be doing everything via the command line so that you can understand the steps in detail. If you are using an IDE like VisualStudio Code , Eclipse , or the hosted Eclipse Che IDE (CodeReady Workspaces on OpenShift), the Eclipse Codewind extension provides the same function to create, run, debug, and test these applications without using the command line or leaving the IDE.","title":"Lab Tasks"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#execute-lab-tasks","text":"Log in to the Workstation VM and Get Started a) If he VMs are not already started, start them by clicking the Play button b) After the VMs are started, click the Workstation VM icon to access it. The Workstation Linux Desktop is displayed. You will execute all the lab tasks on this VM. If requested to login to the OS, use credentials: ibmdemo / passw0rd","title":"Execute Lab Tasks"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#add-the-collection-hub-repo-to-appsody","text":"Appsody uses sharable technology stacks for rapid development of quality microservice-based applications. Each Application Stack is pre-configured with popular open source technologies. Application Stacks include a base container image (UBI from Red Hat for all Stacks in CP4Apps) and project templates which act as a starting point for your application development. In this task, you will add the local Collection Hub built by the enterprises Solution Architect to Appsody. From the Workstation desktop, open a Terminal window by clicking its icon on the Desktop Tool Bar. In the Terminal window, issue the following command to add the Collection Hub to the local Appsody repo list: appsody repo add kabanero https://github.com/kabanero-io/collections/releases/download/0.3.5/kabanero-index.yaml Where kabanero is the collection repo name The kabanero repo is now added to Appsody. To verify it, issue command: appsody repo list The command returns the list of repos in the collection To view the available stacks included in the kabanero repo, run the following command: appsody list kabanero This will give you a list of the available stacks. Notice that the kabanero repo contains Application Stacks for specific versions of Nodejs, Springboot, Microprofile. Set the kabanero repo as the default Appsody repo with command: appsody repo set-default kabanero The default Appsody repo is now set. You will use the nodejs-express stack to develop the insurance quote front-end application.","title":"Add the Collection Hub repo to Appsody"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#create-the-frontend-application-and-run-it-locally","text":"The frontend application is written in Node.js Express. First let's initialize a new project that uses the Node.js Express stack. In the terminal window, create a directory named quote-frontend . Create a new node.js application using appsody init command shown below: mkdir /home/ibmdemo/quote-frontend cd quote-frontend appsody init kabanero/nodejs-express The new node.js application is now successfully initialized, as illustrated below. After the appsody init command completes, launch Visual Studio Code (VS Code) editor with the command below to view the content of the directory. code . The Application Stack has a simplified project structure on disk. Note that this project doesn't contain a Dockerfile, Node, Express, or many of the other files that you might expect in a cloud-native Express project. With Application Stacks built using Appsody these files - along with many best practices and opinionated decisions are provided by the stack, resulting in automatically following best practices and a much simpler view for the developer. Here is an example of what\u2019s included in an Application Stack: In this lab, you are going to replace the starter code created in the init process with the insurance quote frontend application code. a) First, edit the package.json file and add the following to the dependencies section: \"dependencies\": { \"body-parser\": \"^1.19.0\", \"config\": \"^3.2.0\", \"express-validator\": \"^6.2.0\", \"pug\": \"^2.0.0\", \"request\": \"^2.88.0\" Refer to the screen shot below to see where to place the text in the package.json file. Click File > Save All to change the change Then, Exit VS Code. Now, from the Terminal window, copy the insurance quote frontend application code files from the /home/ibmdemo/cp4a-labs/lab8/exercise-frontend directory to your Appsody project: cp -R /home/ibmdemo/cp4a-labs/lab8/exercise-frontend/* . The resulting directory structure of your project is illustrated below: Stop Transformation Advisor Docker container that is running on port 3000, which conflicts with Appsody. This is just to work around our lab environment configuration. /home/ibmdemo/ta-local/transformationAdvisor/scripts/stopTransformationAdvisor.sh Start the application locally, using the following command: appsody run You can see the output of the application by launching a web browser window and navigating to http://localhost:3000 The Insurance Quote form is displayed, as illustrated below You can fill in the form and hit the button to submit it and a response will appear like this: . Note: In this case, the frontend application is not sending a request to the backend application. Instead it is configured to use a mock endpoint for testing purposes in development mode. This works as follows. quote.js uses the config module to get the value for the backend URL. When the application runs in development mode, the config module uses config/development.json to find the value for the backend URL. This file sets the URL to the mock endpoint in the config/development.json file. { \"backendUrl\": \"http://localhost:3000/quote/test\" } When the application runs in production mode (which we will see later), the config module uses config/custom-environment-variables.json file to find the value for the backend URL. This file sets the URL from the BACKEND_URL environment variable. { \"backendUrl\": \"BACKEND_URL\" } Go back to the Terminal window and enter Ctrl+C to stop the application. Exit the VS Code UI, if it is still open Congratulations! You have successfully used capabilities built into IBM Cloud Pak for Applications to create, build, and deploy a cloud native application in a local containerized environment. * In the next section, you will leverage the out-of-the-box Pipelines included with IBM Cloud Pak for Applications to build, scan, and deploy the cloud- native application to OpenShift. More information on Appsody, an Open-Source project under the Kabanero Open-Source project: Appsody documentation on CLI commands: https://appsody.dev/docs/using-appsody/cli-commands/ Appsody documentation for Testing stacks: https://appsody.dev/docs/stacks/test","title":"Create the frontend application and run it locally"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#lets-recap-what-we-have-done-so-far","text":"First, you created a new local collection hub based on a public collection hub You listed the available application stacks in the local collection hub You made your \u201ckabanero\u201d stack the default stack in the local collection hub You initialized the \u201cnodejs-express\u201d application stack in the dev.local repo You used Appsody CLI to \u201c run \u201d the \u201cfront-end\u201d nodejs-express application locally in containers, using Appsody CLI. This also built the application using the out-of-the-box-resources in the stack. Finally, you used the web browser to test the \u201cfront-end\u201d nodejs-express application stack from the kabanero collection.","title":"Let\u2019s recap what we have done so far"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#deploy-the-quote-frontend-application-to-openshift-with-the-cicd-pipeline","text":"In this section, you will deploy the frontend application to OpenShift. You will use a Pipeline based on Tekton to deploy the application in a controlled and secure manner onto the OpenShift Container Platform. Note: Tekton is the Open-Source upstream project for RedHat OpenShift Pipelines. Enterprise solution architects generally will work with Cloud Operators who manage OpenShift production clusters and developers to create approved pipelines with steps that meet their enterprise\u2019s needs. Navigate to your quote-frontend directory. We first create the deployment yaml as follows: cd /home/ibmdemo/quote-frontend appsody deploy --generate-only This creates a file named app-deploy.yaml in your project folder. It will take a couple of minutes for the task to complete. On-line documentation for Appsody deploy: https://appsody.dev/docs/cli-commands/#appsody-deploy Now we need to tell the frontend application how to talk to the backend application. The frontend will look at an environment variable BACKEND_URL to find the address of the backend, so we will set this in the application Custom Resource (CR). a) Exit VS Code UI, if it is still running. b) Then, relaunch VS Code from the current directory of your quote-frontend app: code . c) From the VS Code edit, open the app-deploy.yaml file and add the following section as shown below: env: - name: BACKEND\\_URL value: <http://quote-backend:8080/quote> Notes: a. Your line numbers may be different if the template code has changed since the screenshot. b. This is yaml file, so indentation matters. And No TABS! d) Save the change by clicking File > Save All e) Exit VS Code. Create a Git repo for the application. You will push the application to your GitHub repo. This step requires that you have a public GitHub account. If you do not have a GitHub account, create a free account at: https://github.com a) From the web browser window, click the GitHub bookmark to launch it. b) Log in to your existing GitHub account. If you do not have one, you can sign up to get a free account. c) From your GitHub page, click Add>New repository to create a new repo. d) Enter the Repository name as quote-frontend, select the Public option and click Create repository. e) Your repository is created. Write down the GitHub repo URL link of the repository, you are going to use it later. f) From the Terminal window, push your application code to the Git repo, using the following commands: cd /home/ibmdemo/quote-frontend git init git add . git commit -m \"first commit\" git remote add origin https://github.com/<your Github User Name>/quote-frontend.git Note: where <your Github User Name> is your GitHub user name. git push -u origin master Note: When prompted, enter your GitHub username and password Refresh your GitHub Repo and verify the contents have been published to the quote-frontend repo","title":"Deploy the quote-frontend application to OpenShift with the CI/CD Pipeline"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#add-the-pipeline-resources-to-openshift","text":"Login to the OpenShift Cluster oc login -u admin -p passw0rd Set up a project namespace. OpenShift applications are deployed within a project. The first step in the deployment process is to create a new or select an existing project. In this lab, we are going to use an existing project called kabanero . From the Terminal window, issue the following commands: oc project kabanero Note: You are now working in the \u201c kabanero \u201d project in OpenShift. Create a new Tekton pipeline CR. (Kubernetes Custom Resource) Note: In general, developers don\u2019t create the pipelines. They should be setup for them with all the tasks their solution architect and cloud operator teams require. In this lab, you will act as the operator to set the pipeline CR and get familiar with the process. a) Copy the sample pipeline CR file pipeline-resources.yaml from /home/ibmdemo/cp4a-labs/lab8/exercise-frontend/ directory to /home/ibmdemo/quote-frontend directory, using the following commands: cd /home/ibmdemo/quote-frontend cp /home/ibmdemo/cp4a-labs/lab8/exercise-frontend/pipeline-resources.yaml . b) Return to the VS Code editor, or launch it, if it is not running. code . c) Click pipeline-resources.yaml file to open it, change the GitHub user to your Github username d) Click File > Save All to save the change. e) Exit VS Code f) Deploy the pipeline resource to the OCP cluster kabanero namespace, using the following commands: cd /home/ibmdemo/quote-frontend oc apply -f pipeline-resources.yaml -n kabanero","title":"Add the Pipeline resources to OpenShift"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#deploy-the-quote-frontend-application-to-openshft-using-the-tekton-piepline","text":"Access the Tekton Dashboard to setup the PipelineRun In order to provide control and consistency at scale, all build and deploy steps must go through the pipeline. So, in an enterprise setting, webhooks would be setup by the operations or solution architect that monitor the developers Git repos and trigger the PipelineRun during commit/push actions. Due to network connectivity limitations with the lab infrastructure, we are asking you to create a manual PipelineRun to simulate the webhook completing that step, enabling the rest of the lab flow to continue. a) Go back to the web browser window and launch the Tekton Dashboard by clicking its bookmark. b) Click Log in with OpenShift to continue c) In the login page, click kube:admin field, select the kubeadmin user name and click Log in . Create the PipelineRun and Run the Pipeline to deploy the application. a) From the Tekton Dashboard page, you can click the Pipelines link to view all available pipelines, or you can click PipelineResources to view the two resourced you just created. b) We are going to use the PipelineRun button to deploy the quote-frontend application, so click Create PipelineRun in the PipelineRuns page. c) In the PipelineRun page, set the parameters as shown in table below. Then click Create . Note: Refer to the screenshot below the table, if additional clarity is needed for input parameters. Namespace kabanero Pipeline nodejs-express-build-push-deploy-pipeline git quote-frontend-git image quote-frontend-image Event-header demo Service Account kabanero-operator d) The pipelinerun is created, click the its link to view the run status: e) The pipeline is running. It has several tasks, including build-push , image scan and deploy . Each task contains multiple stages. When the stage is running, you can click its link and view its logs , status and details. When the PipelineRun completes, your application is deployed to OpenShift cluster. The pipeline will take several minutes to complete all of the tasks. It downloads and installs a lot of pf modules during the build phase. It took about 15 minutes in my testing! Once the Pipeline completes, the final step in the lab is to validate the deployment to OpenShift. To verify the deployment to OpenShift a) To view your deployment in the OCP cluster, go back to the web browser window and click the OpenShift Web Console bookmark. b) If prompted to login, click the htpassword link and login using admin / passw0rd c) From the OpenShift Web Console, navigate to Workloads>Deployments and go to the kabanero project d) Click the quote-frontend application link to view its deployment details e) Next, select Networking>Routes section and click the quote-frontend application URL to lunch it. The Quote application Web page is displayed, as illustrated below:","title":"Deploy the quote-frontend application to OpenShft using the Tekton Piepline"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#summary","text":"In this lab, you have learned how to create a cloud native microservice application using customized Application Stacks provided by Cloud Pak for Applications. These stacks are built using an open source project called Appsody. You can learned how to use the CLI provided by Appsody to create projects, do iterative local development and deploy applications to OpenShift Pipelines provided with Cloud Pak for Applications. IBM Cloud Pak for Application s makes it faster and easier for developers to create and deploy cloud native applications on OpenShift using Application Stacks that also provide enterprise governance for cloud native development.","title":"Summary"},{"location":"ITC-21_v3.13_CreateDeployCloudNativeICP4A_LABGUDE-tgf/#congratulations","text":"You have successfully completed the lab \u201cCreate and Deploy a cloud native application to IBM Cloud Pak for Applications\u201d.","title":"Congratulations!"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/","text":"Session ID: ITC-22 How to create policy driven application placement with IBM Multicloud manager Lab. Enable and Deploy Applications with Policy Enforcement On the journey to cloud, enterprise customers are deploying multiple Kubernetes clusters and OpenShift clusters. As the number of clusters increase, it will be difficult to deploy and manage the applications across all these clusters. Enabling the applications to deploy to various Kubernetes/OpenShift clusters meeting certain criteria will ensure management of the life cycle of the applications. To achieve this, all the applications which are packaged as helm charts will be hosted in one or more repositories. The repositories, which contain the application packages are defined as channels which broadcast across the clusters which are managed. Now, if you want to deploy an application, then you define a subscription to the channel with the name of the application (one or more) you want to deploy. This is similar to the subscription model of TV channels. Here is a schematic describing how we now subscribe to a channel which has connected content. Much like a TV broadcast model where households subscribe to different channels for different content streams. Schematically the diagram below describes the process. This lab walks you through the process of how to define channels (namespace or helm repositories) and then subscribe to the channels to deploy the applications across multiple clusters depending on the subscription and placement policies. Objective The objectives of this lab are to: Learn how to define channels for the repositories holding application packages Learn how to subscribe to the channel to deploy an application Change the placement policy rule and validate the policy is enforced Learn CLI commands that are useful to gain insights into the state of the MCM application Prerequisites The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands, Kubernetes commands, helm charts and YAML. Concepts of management of Multicloud Management (managing multiple Kubernetes clusters) A Management Cluster is available and ready to use. An OpenShift cluster that needs to be managed The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates useful information that is good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information. What is Already Completed Two OpenShift Clusters pre-deployed where one cluster acts as Management Cluster and other acts as managed cluster. On the management cluster, the management software is loaded. The VMs with names ( master1, worker1, worker2 ) are part of management cluster. It\u2019s an Openshift 4.2 cluster. The VMs with names OCP constitute the managed cluster. It\u2019s an Openshift 3.11 all-in-one cluster. The Developer VM is the one you will use to access and work with in this lab. The DNS VM works to support management cluster and cross-cluster DNS. The login credentials for the Developer VM are: User ID: ibmuser Password: passw0rd (this is the same sudo password) Lab Tasks During this lab, you will complete the following tasks: Log in to the Cluster to ensure they are operational Define a channel definition (namespace based, and helm chart based) so that subscribers can pull the applications Subscribe to the channel Verify the application is deployed successfully, and test the application Modify the placement policy to automatically deploy the application to an alternative managed cluster Explore some useful CLI commands to determine state of the application and MCM resources Check the Environment Log in to the Workstation VM and get started If the VMs are NOT already running, launch the lab VMs by clicking the Play button highlighted in the illustration of VMs above. After the VMs are started (takes few minutes), click the Developer VM icon to access it. The Developer Linux Desktop is displayed. You will execute all the lab tasks on this VM. Check the Environment In this task, let\u2019s check the Management environment to ensure it is up and its pods are all running. To check the environment, login to the \u201c Developer \u201d VM using userid: ibmuser and password: passw0rd . The workstation has the required tools oc , cloudctl and helm tools installed. Launch a terminal window and login to Management OpenShift Cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Verify all of the management pods are running in the Management cluster oc get pods -n kube-system |grep -v Running|grep -v Completed oc get pods -n multicluster-endpoint | grep -v Running|grep -v Completed There should NOT be any pods listed in the results. That indicates all the pods are in the Running state. If there are pods listed, and not running, contact the instructor, as the management cluster may not be running correctly. Get the route for the Multicloud Management Console (Named icp-console ) oc -n kube-system get routes a) Select the value of icp-console.apps.demo.ibmdte.net like highlighted below Launch a browser and log in to the Mutlicloud Management Console URL: https://icp-console.apps.demo.ibmdte.net/multicloud/welcome Lognin using the following credentials: userid: admin password: passw0rd The console, welcome page should be displayed ( read below, if you do not get this page ) If you see the incorrect page: \u201c IBMCloud Pak \u201d screen, Rerun the URL link again. This appears to be a bug in our environment Navigate to Menu -> Observe Environments -> Overview. This view displays the overall view of all the clusters that are managed. From here, you can navigate to the Clusters, Nodes and the Pods views. As you scroll down, the overall state of the clusters is displayed. In the console, go to Menu -> Automate infrastructure -> Clusters . This view shows the managed clusters. Click the local-cluster cluster name to view the details of the cluster. The overview tab displays the details of the cluster and the nodes tab displays the details of nodes with its labels. Subscription to a Namespace Channel In this task, you define a namespace subscription channel and subscribe to that channel. Channels (Channel.app.ibm.com) define a namespace within the hub cluster and point to a physical place where resources are stored for deployment; such as an object store, Kubernetes namespace, or Helm repository. In this lab, the channel is of type \u201c namespace \u201d meaning that the yaml you create will be deployed and stored in OpenShift namespaces, rather than in a Helm chart or Object store. Channels are custom resource definitions that can help you streamline deployments and separate cluster access. Clusters can subscribe to channels for identifying the deployables to deploy to each cluster. Deployables within a channel can be accessed by only the clusters that subscribe to that channel. You will perform the following tasks in this section Define the Modresort application as a channel Deploy the channel Define the namespace subscription model Deploy the subscription Check the application Define the application as a channel The modresort application is a simple application with only one component. Modresort: A WebSphere Liberty Java application - available in Dockerhub at kpostreich/modresort Construct the yaml files for the Channel definition and Deployables for the Modresort application In this section, you will create the yaml resources required to construct the Channel and Deployables for the modresort application. Finally, you will deploy the artifacts and its resources to an MCM managed cluster. Note: The yaml files that you will create in this section are also stored in an \u201cInstructor\u201d folder: /home/ibmuser/instructor/modresortchan You can use the snippets below to create the yaml files. Or, you can copy / paste the yaml files from the \u201cInstructor\u201d folder. It is your choice. First, we need to create a working directory to create the resource definition files (yaml). Launch a command window, create a new directory, and then change directory to the new directory: /home/ibmuser/student/modresortchan mkdir -p /home/ibmuser/student/modresortchan cd /home/ibmuser/student/modresortchan In the modresortchan folder, use gedit or vi to create the channel definition . Name the file \u201c channel.yaml\u201d The spec collection defines the type of the channel. The contents of this file are shown below: apiVersion: app.ibm.com/v1alpha1 kind: Channel metadata: name: modresort-devchan namespace: modresort-entitlement labels: app: modresortchan spec: type: Namespace pathname: modresort-entitlement Notice the spec:type is \u201c Namespace \u201d. The Kubernetes namespace where the channel is deployed will be \u201cmodresort-entitlement\u201d . You will create this namespace later in the lab, when we deploy the channel definition to that namespace. Channels (Channel.app.ibm.com) define a namespace within the hub cluster and point to a physical place where resources are stored for deployment. There are three types of channels. Each channel differs based on the type of source location where resources are stored: Kubernetes namespace . Uses deployables to store Kubernetes resource templates. Subscription for this type of channel retrieves and deploys the template. The namespaces that the channel monitors for new or updated deployables must be on the hub cluster. Object store . Stores Kubernetes resource YAML files. Each YAML file includes the template portion for one resource, not the full deployable object. An object store can be populated with a deployable object that is on the hub cluster or directly from a continuous integration pipeline, or by including the required YAML files into the object store. Helm repository . Stores Helm charts. For information about how to structure your charts, Refer to the on-line documentation for details on the construct of the channel definition: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_channels.html The modresort application component consists of a Kubernetes deployment resource definition and a Kubernetes service resource definition. To enable these components to be used by the channel subscription, each of the resources need to be wrapped by a new custom resource definition (CRD) called Deployable . For the application to properly function with policy enforcement, this CRD is required. In the next step, create the Deployable resource for the modresort application. In the modresortchan folder, create the Deployable file with name deployable.yaml using the content below: apiVersion: app.ibm.com/v1alpha1 kind: Deployable metadata: name: devchan-modresortchan-deployment namespace: modresort-entitlement annotations: app.ibm.com/is-local-deployable: \"false\" labels: app: modresortchan component: main package: modresort spec: template: kind: Deployment apiVersion: apps/v1 metadata: name: devchan-modresortchan-deployment spec: selector: matchLabels: app: modresortchan release: modresort-devchan tier: frontend replicas: 1 template: metadata: labels: app: modresortchan release: modresort-devchan tier: frontend spec: containers: - name: frontend image: \"kpostreich/modresort:1.0\" imagePullPolicy: Always ports: - containerPort: 9080 env: - name: GET_HOSTS_FROM value: dns - name: WLP_LOGGING_CONSOLE_FORMAT value: json - name: WLP_LOGGING_CONSOLE_LOGLEVEL value: info - name: WLP_LOGGING_CONSOLE_SOURCE value: message,trace,accessLog,ffdc --- apiVersion: app.ibm.com/v1alpha1 kind: Deployable metadata: name: devchan-modresortchan-service namespace: modresort-entitlement annotations: app.ibm.com/is-local-deployable: \"false\" labels: app: modresortchan component: main package: modresort spec: template: kind: Service apiVersion: v1 metadata: name: devchan-modresortchan-service labels: app: modresortchan chart: modresortchan-0.1.0 release: modresort-devchan heritage: Tiller spec: type: NodePort ports: - port: 9080 selector: app: modresortchan release: modresort-devchan tier: frontend Take note of a few elements of the Deployable definition: There are two Deployables defined in the yaml file that wrap the modresort Kubernetes resources One for the modresort deployment One for the modresort service The Deployment and Service specs are specific to the Modresort application deployment to an OpenShift Cluster. For example, the Deployment refers to the location where the Docker image will be pulled, number of replicas, etc For example, the Service refers to the service port, and defines a NodePort for externally accessing the Modresort app in a test environment. Refer to the on-line documentation for details on the construct of the Deployable definition: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_deployables.html Create a subscription The subscription to a channel package contains - Application Definition - Placement Rules Definition - Subscription Definition Applications (Application.app.k8s.io) in IBM Multicloud Manager are used for grouping application components. Placement rules (PlacementRule.app.ibm.com) define the target clusters where deployables can be deployed. You can use placement rules to help you facilitate the multi-cluster deployment of your deployables. Placement rules can be referenced by deployables and subscriptions. Subscriptions (Subscription.app.ibm.com) are sets of definitions that identify deployables within channels by using annotations, labels, and versions. The subscription controller can monitor the channel for new or updated deployables, such as an updated Helm release or new Kubernetes deployable object. Then, the controller can download the Kubernetes deployable object or Helm release directly from the source location (Helm repository, object store, or namespace) to the target managed clusters. Create the resources for \u201cApplication\u201d, \u201cPlacementRule\u201d, and \u201cSubscription\u201d You will create these additional yaml resources for the application in a new folder for the application resources. They will also be deployed to a separate namespace than the Channel and Deployable resources. Note: The yaml files that you will create in this section are also stored in an \u201cInstructor\u201d folder: /home/ibmuser/instructor/modresortapp You can use the snippets below to create the yaml files. Or, you can copy / paste the yaml files from the \u201cInstructor\u201d folder. It is your choice. Create a new directory for your application modresortapp , and change to the new directory: /home/ibmuser/student/modresortapp mkdir /home/ibmuser/student/modresortapp cd /home/ibmuser/student/modresortapp Create the application.yaml which defines the grouping of the application components. Notice the resource kind is \u201cApplication\u201d, and the Component kinds is a \u201cSubscription\u201d. apiVersion: app.k8s.io/v1beta1 kind: Application metadata: name: modresort101-modresortapp namespace: modresort-project labels: app: modresortapp spec: selector: matchExpressions: - key: release operator: In values: - modresort101 componentKinds: - group: app.ibm.com kind: Subscription Refer to the on-line documentation for details on configuring the Application resource: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/app_lifecycle.html> Create placementrules.yaml file. This contains the definition for placement of the components as selected by the user. In the definition you will find placement rule for the modresort component. The selection criteria for where the application will be deployed is based on the \u201c environment \u201d cluster label. The value of \u201c Dev \u201d means the Modresort app will be deployed to the local cluster. A value of \u201c Prod \u201d for the environment label, will result in the Modresort application being deployed to the managed cluster (OCP). apiVersion: app.ibm.com/v1alpha1 kind: PlacementRule metadata: name: modresortapp101-modresortapp namespace: modresort-project labels: app: modresortapp release: modresort101 spec: clusterReplicas: 1 clusterLabels: matchLabels: environment: Prod Refer to the on-line documentation for details on configuring the PlacementRule resource: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_placement_rules.html Create subscription.yaml file. This contains the details for relating the placement rule definition with the application specification. In the definition file, there is a subscription definition for the modresort application component, which references the modresort application channel . Through this subscription, the modresort application will be deployed to the cluster based on the placement rule that is referenced here in the subscription . apiVersion: app.ibm.com/v1alpha1 kind: Subscription metadata: name: modresort101-modresortapp namespace: modresort-project labels: app: modresortapp release: modresort101 spec: channel: modresort-entitlement/modresort-devchan name: \"\" packageFilter: version: \">=1.x\" labelSelector: matchLabels: package: modresort component: main placement: placementRef: name: modresortapp101-modresortapp kind: PlacementRule group: app.ibm.com overrides: - clusterName: \"/\" clusterOverrides: - path: \"metadata.namespace\" value: default Refer to the on-line documentation for details on configuring the Subscription resource: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_subscriptions.html This completes enabling an existing application with policies so that they can be deployed to any Kubernetes managed cluster via MCM placement policies. Deploy the application that is built In this section, you will deploy the application components to their respective Kubernetes namespaces, using the yaml files that you created in the previous steps. Launch the command window and log in to the hub OpenShift cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Use the icp-console URL to login to the Multicloud Management console CLI cloudctl login -a https://icp-console.apps.demo.ibmdte.net/ -u admin -p passw0rd -n default As the channel is namespace based, create two namespaces (projects in OpenShift) where one is for the subscription and the other is for the channel . Run the following commands to create the projects in OpenShift oc new-project modresort-project oc new-project modresort-entitlement Deploy the modresortchan application (channel definition) to the modresort - entitlement project oc project modresort-entitlement OUTPUT: Already on project \"modresort-entitlement\" on server \"https://api.demo.ibmdte.net:6443\" cd /home/ibmuser/student oc apply -f modresortchan Output of the above command will be similar to the illustration below. The output shows that Channel and Deployables are created Now deploy the subscription to the \u201c modresort-project\u201d namespace. The subscription is deployed to a different namespace than the channel. Never deploy the subscription to the same namespace as the channel. Hence, deploy the subscription to the other namespace, modresort - project oc project modresort-project OUTPUT: Now using project \"modresort-project\" on server \"https://api.demo.ibmdte.net:6443\" oc apply -f modresortapp Continue to the next section of the lab, which walks you through validating the application deployment. Validate the Application As the channel and the subscription are deployed, now you can validate if the application is running. Go to the console: https://icp-console.apps.demo.ibmdte.net/multicloud/welcome using username: admin and password: passw0rd Check the \"Applications view:\" menu -> Manage Applications. You should see the application listed . Note: It may take a couple of moments for the application to be available. Be patient and refresh the page until the application is displayed in the UI. Click on the modresort101-modresortapp application link to get to the application Overview page. a) Then, scroll down to view the application diagram You will see the Resource Topology for the modresort application. Note: It may take a couple of minutes for the application to > deploy to the cluster. When the application is successfully deployed, via the subscription, the pod in the application topology view will have a GREEN icon, as illustrated below. Note: After a couple of minutes, if your pod has a YELLOW icon next to the pod, indicating it is in an unknown state, you will need to troubleshoot your yaml files for correctness, and redeploy the resources. Check your subscriptions to verify they are subscribed and propagated oc get subscription.app.ibm.com --all-namespaces The subscription from the mcm hub should be in the \u201c Propagated\u201d state. Check your subscription on the \u2018 ocp\u2019 node (Prod) cluster to verify the subscription in in the subscribed \u201d state. a) Open a new terminal window b) ssh into the ocp node using the credentials ibmuser / passw0rd ssh ocp -l ibmuser when prompted for the password enter: passw0rd c) Login to the OCP cluster (This is the Prod cluster) oc login -u admin -p passw0rd https://ocp.demo.ibmdte.net:8443 d) Check the status of the subscription in the \u201c default \u201d namespace in the OCP cluster. Note: The subscription.yaml file you created earlier defined the namespace for the Modresort application to be the \u201c default \u201d namespace in the target cluster. oc get subscription.app.ibm.com -n default The subscription in \u201c default \u201d namespace should be in the \u201c Subscribed \u201d state Check the modresort pod state in the \u201c default \u201d namespace in the OCP cluster oc get pods -n default You should see the devchan-modresort-deployment pod running in the \u201c default \u201d namespace Test the modresorts application from the browser Note: Ensure you run the following command from the OCP ssh terminal window a) Use the \u201coc get services -n default\u201d command to get the devchan-modresortchan-service NodePort. oc get services -n default b) From a browser: http://10.0.0.10:\\<NodePort>/resorts Type \u201c exit \u201d to exit from the ssh of the OCP cluster Close the 2 nd terminal window where you connected via ssh to the OCP cluster Congratulations! You have successfully deployed the modresorts application to the local OpenShift Cluster, using the MCM subscription based \u201cPlacementrule\u201d deployment model. Explore some valuable cli commands to view the state of the deployed resources Using the cli, you can check quite a few things about the deployed resources Ensure you are logged into to the hub openshift cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Using the cli, check the placementrules using the \u201cget placementrules\u201d command: oc get placementrules --all-namespaces Check where the subscription landed, in the status section of the output from the following command: oc get placementrules modresortapp101-modresortapp -n modresort-project -o yaml Enter the command below to check all the deployables . Then you can check the deployables for any errors by using oc logs\u2026. And oc describe commands as needed for troubleshooting. oc get deployables.app.ibm.com --all-namespaces Command to check the registered clusters oc get **cluster**s.**cluster**registry.k8s.io --all-namespaces Similarly, to check any resource, you can use oc api-resources to display all the resources and use any resource. For example: oc api-resources | grep deployable From the above example, to get deployables, you specify the resource including its APIGROUP as illustrated below: oc get deployables.app.ibm.com --all-namespaces Here is the example and output from getting the MCM Application resources oc api-resources | grep application From the above example, to get deployables, you specify the resource including its full API resource name, as illustrated below: oc get applications.app.k8s.io --all-namespaces Congratulations! You are now familiar with several important MCM specific CLI commands that can help you gain insights into the state of your MCM managed applications. Modify placement rules to move the application to the \"dev\" managed cluster The previous portions of the lab deployed the modresort application to the managed cluster labeled as \u201c Prod \u201d. You can work with the placement rule to see how MCM can easily redeploy the application to a different cluster based on selection criteria in the placement rule. In this section, you will modify the placement rule to redeploy the modresort application to the local cluster, labeled Dev \u201d. From the MCM clusters view in the MCM Hub console, you can see all the labels the admin set on the managed clusters. These are the labels on the two managed clusters in the lab environment. Notice the environment label. That is what you defined in the placement rule to deploy the application to the \u201cProd\u201d cluster. The environment label for the local managed cluster is environment=Dev Dev Cluster Prod Cluster Modify Placement Rules to deploy modresorts to the \"Dev\" managed cluster Return to the Mutlicloud Management Console in the Browser. If you closed the console, you can login again using the URL and credentials below. URL: https://icp-console.apps.demo.ibmdte.net userid: ibmuser password: passw0rd From the menu, navigate to the \u201c Manage Applications\u201d view, where you will see the modresort application listed Click on the modresort101-modresortapp in the list. From the application Overview page, scroll down to the Resource Topology section You can edit the placement policy using the Edit Topology link located in the lower right corner, and then clicking on the placement rules icon as illustrated below. a) Click on the Edit Topology link to view the yaml file. b) Click on the Placement Rules icon, which will take you directly to the placement rules section of the yaml file. c) Place curser on edge of the Editor view, and drag it left, widening the view area. d) Notice the Modresort application is currently deployed to the cluster labeled as \u201c Prod \u201d. e) In the Yaml view, on approximately line #60, modify the environment from Prod to Dev Note: Case Matters \u2013 Upper case \u201cD\u201d f) Click the Update icon to save the changes to the yaml g) Click the Close editor link to close the yaml editor h) After a couple of minutes, the modresort application will be redeployed to the \u201c Dev \u201d managed cluster. Refresh the application view , then verify the Resource Topology section shows that application now deployed to \u201c Dev \u201d, which is the local-cluster cluster. a) The pod, once redeployed to the local cluster, will show in the running state, indicated by a green icon, as illustrated below: It may take a couple of minutes for the application to be redeployed to the local cluster. Login to the local cluster CLI (hub cluster) to run oc commands to verify the application state. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 You can use the following command to see if the modresort pod got deployed and its current state of the pod and subscription, and placement rules oc get pods -n default oc get subscription.app.ibm.com -n default</p> oc get placementrules modresortapp101-modresortapp -n modresort-project -o yaml Note: Output from the above commands are illustrated below: Test the modresorts application from the browser a) Use the oc get services -n default command to get the devchan-modresortchan-service NodePort. oc get services -n default From a browser: http://10.0.0.4:\\<NodePort>/resorts Note: 10.0.0.4 is the \u201cMaster\u201d node of the local-cluster cluster (Dev) Congratulations! You have successfully re-deployed the modresorts application to the \u201cProd\u201d managed OpenShift Cluster, using the MCM subscription based \u201cPlacementrule\u201d deployment model. 9 Summary In this lab, you have learned how to enable an existing application definition with channel definition, subscription and placement rules. Congratulations! You have successfully completed the lab \u201cApplication Deployment\u201d","title":"***Session ID: ITC-22***"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#session-id-itc-22","text":"","title":"Session ID: ITC-22"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#how-to-create-policy-driven-application-placement-with-ibm-multicloud-manager","text":"","title":"How to create policy driven application placement with IBM Multicloud manager"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#lab-enable-and-deploy-applications-with-policy-enforcement","text":"On the journey to cloud, enterprise customers are deploying multiple Kubernetes clusters and OpenShift clusters. As the number of clusters increase, it will be difficult to deploy and manage the applications across all these clusters. Enabling the applications to deploy to various Kubernetes/OpenShift clusters meeting certain criteria will ensure management of the life cycle of the applications. To achieve this, all the applications which are packaged as helm charts will be hosted in one or more repositories. The repositories, which contain the application packages are defined as channels which broadcast across the clusters which are managed. Now, if you want to deploy an application, then you define a subscription to the channel with the name of the application (one or more) you want to deploy. This is similar to the subscription model of TV channels. Here is a schematic describing how we now subscribe to a channel which has connected content. Much like a TV broadcast model where households subscribe to different channels for different content streams. Schematically the diagram below describes the process. This lab walks you through the process of how to define channels (namespace or helm repositories) and then subscribe to the channels to deploy the applications across multiple clusters depending on the subscription and placement policies.","title":"Lab. Enable and Deploy Applications with Policy Enforcement"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#objective","text":"The objectives of this lab are to: Learn how to define channels for the repositories holding application packages Learn how to subscribe to the channel to deploy an application Change the placement policy rule and validate the policy is enforced Learn CLI commands that are useful to gain insights into the state of the MCM application","title":"Objective"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#prerequisites","text":"The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands, Kubernetes commands, helm charts and YAML. Concepts of management of Multicloud Management (managing multiple Kubernetes clusters) A Management Cluster is available and ready to use. An OpenShift cluster that needs to be managed The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates useful information that is good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information.","title":"Prerequisites"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#what-is-already-completed","text":"Two OpenShift Clusters pre-deployed where one cluster acts as Management Cluster and other acts as managed cluster. On the management cluster, the management software is loaded. The VMs with names ( master1, worker1, worker2 ) are part of management cluster. It\u2019s an Openshift 4.2 cluster. The VMs with names OCP constitute the managed cluster. It\u2019s an Openshift 3.11 all-in-one cluster. The Developer VM is the one you will use to access and work with in this lab. The DNS VM works to support management cluster and cross-cluster DNS. The login credentials for the Developer VM are: User ID: ibmuser Password: passw0rd (this is the same sudo password)","title":"What is Already Completed"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#lab-tasks","text":"During this lab, you will complete the following tasks: Log in to the Cluster to ensure they are operational Define a channel definition (namespace based, and helm chart based) so that subscribers can pull the applications Subscribe to the channel Verify the application is deployed successfully, and test the application Modify the placement policy to automatically deploy the application to an alternative managed cluster Explore some useful CLI commands to determine state of the application and MCM resources","title":"Lab Tasks"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#check-the-environment","text":"","title":"Check the Environment"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#log-in-to-the-workstation-vm-and-get-started","text":"If the VMs are NOT already running, launch the lab VMs by clicking the Play button highlighted in the illustration of VMs above. After the VMs are started (takes few minutes), click the Developer VM icon to access it. The Developer Linux Desktop is displayed. You will execute all the lab tasks on this VM.","title":"Log in to the Workstation VM and get started"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#check-the-environment_1","text":"In this task, let\u2019s check the Management environment to ensure it is up and its pods are all running. To check the environment, login to the \u201c Developer \u201d VM using userid: ibmuser and password: passw0rd . The workstation has the required tools oc , cloudctl and helm tools installed. Launch a terminal window and login to Management OpenShift Cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Verify all of the management pods are running in the Management cluster oc get pods -n kube-system |grep -v Running|grep -v Completed oc get pods -n multicluster-endpoint | grep -v Running|grep -v Completed There should NOT be any pods listed in the results. That indicates all the pods are in the Running state. If there are pods listed, and not running, contact the instructor, as the management cluster may not be running correctly. Get the route for the Multicloud Management Console (Named icp-console ) oc -n kube-system get routes a) Select the value of icp-console.apps.demo.ibmdte.net like highlighted below Launch a browser and log in to the Mutlicloud Management Console URL: https://icp-console.apps.demo.ibmdte.net/multicloud/welcome Lognin using the following credentials: userid: admin password: passw0rd The console, welcome page should be displayed ( read below, if you do not get this page ) If you see the incorrect page: \u201c IBMCloud Pak \u201d screen, Rerun the URL link again. This appears to be a bug in our environment Navigate to Menu -> Observe Environments -> Overview. This view displays the overall view of all the clusters that are managed. From here, you can navigate to the Clusters, Nodes and the Pods views. As you scroll down, the overall state of the clusters is displayed. In the console, go to Menu -> Automate infrastructure -> Clusters . This view shows the managed clusters. Click the local-cluster cluster name to view the details of the cluster. The overview tab displays the details of the cluster and the nodes tab displays the details of nodes with its labels.","title":"Check the Environment"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#subscription-to-a-namespace-channel","text":"In this task, you define a namespace subscription channel and subscribe to that channel. Channels (Channel.app.ibm.com) define a namespace within the hub cluster and point to a physical place where resources are stored for deployment; such as an object store, Kubernetes namespace, or Helm repository. In this lab, the channel is of type \u201c namespace \u201d meaning that the yaml you create will be deployed and stored in OpenShift namespaces, rather than in a Helm chart or Object store. Channels are custom resource definitions that can help you streamline deployments and separate cluster access. Clusters can subscribe to channels for identifying the deployables to deploy to each cluster. Deployables within a channel can be accessed by only the clusters that subscribe to that channel. You will perform the following tasks in this section Define the Modresort application as a channel Deploy the channel Define the namespace subscription model Deploy the subscription Check the application","title":"Subscription to a Namespace Channel"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#define-the-application-as-a-channel","text":"The modresort application is a simple application with only one component. Modresort: A WebSphere Liberty Java application - available in Dockerhub at kpostreich/modresort Construct the yaml files for the Channel definition and Deployables for the Modresort application In this section, you will create the yaml resources required to construct the Channel and Deployables for the modresort application. Finally, you will deploy the artifacts and its resources to an MCM managed cluster. Note: The yaml files that you will create in this section are also stored in an \u201cInstructor\u201d folder: /home/ibmuser/instructor/modresortchan You can use the snippets below to create the yaml files. Or, you can copy / paste the yaml files from the \u201cInstructor\u201d folder. It is your choice. First, we need to create a working directory to create the resource definition files (yaml). Launch a command window, create a new directory, and then change directory to the new directory: /home/ibmuser/student/modresortchan mkdir -p /home/ibmuser/student/modresortchan cd /home/ibmuser/student/modresortchan In the modresortchan folder, use gedit or vi to create the channel definition . Name the file \u201c channel.yaml\u201d The spec collection defines the type of the channel. The contents of this file are shown below: apiVersion: app.ibm.com/v1alpha1 kind: Channel metadata: name: modresort-devchan namespace: modresort-entitlement labels: app: modresortchan spec: type: Namespace pathname: modresort-entitlement Notice the spec:type is \u201c Namespace \u201d. The Kubernetes namespace where the channel is deployed will be \u201cmodresort-entitlement\u201d . You will create this namespace later in the lab, when we deploy the channel definition to that namespace. Channels (Channel.app.ibm.com) define a namespace within the hub cluster and point to a physical place where resources are stored for deployment. There are three types of channels. Each channel differs based on the type of source location where resources are stored: Kubernetes namespace . Uses deployables to store Kubernetes resource templates. Subscription for this type of channel retrieves and deploys the template. The namespaces that the channel monitors for new or updated deployables must be on the hub cluster. Object store . Stores Kubernetes resource YAML files. Each YAML file includes the template portion for one resource, not the full deployable object. An object store can be populated with a deployable object that is on the hub cluster or directly from a continuous integration pipeline, or by including the required YAML files into the object store. Helm repository . Stores Helm charts. For information about how to structure your charts, Refer to the on-line documentation for details on the construct of the channel definition: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_channels.html The modresort application component consists of a Kubernetes deployment resource definition and a Kubernetes service resource definition. To enable these components to be used by the channel subscription, each of the resources need to be wrapped by a new custom resource definition (CRD) called Deployable . For the application to properly function with policy enforcement, this CRD is required. In the next step, create the Deployable resource for the modresort application. In the modresortchan folder, create the Deployable file with name deployable.yaml using the content below: apiVersion: app.ibm.com/v1alpha1 kind: Deployable metadata: name: devchan-modresortchan-deployment namespace: modresort-entitlement annotations: app.ibm.com/is-local-deployable: \"false\" labels: app: modresortchan component: main package: modresort spec: template: kind: Deployment apiVersion: apps/v1 metadata: name: devchan-modresortchan-deployment spec: selector: matchLabels: app: modresortchan release: modresort-devchan tier: frontend replicas: 1 template: metadata: labels: app: modresortchan release: modresort-devchan tier: frontend spec: containers: - name: frontend image: \"kpostreich/modresort:1.0\" imagePullPolicy: Always ports: - containerPort: 9080 env: - name: GET_HOSTS_FROM value: dns - name: WLP_LOGGING_CONSOLE_FORMAT value: json - name: WLP_LOGGING_CONSOLE_LOGLEVEL value: info - name: WLP_LOGGING_CONSOLE_SOURCE value: message,trace,accessLog,ffdc --- apiVersion: app.ibm.com/v1alpha1 kind: Deployable metadata: name: devchan-modresortchan-service namespace: modresort-entitlement annotations: app.ibm.com/is-local-deployable: \"false\" labels: app: modresortchan component: main package: modresort spec: template: kind: Service apiVersion: v1 metadata: name: devchan-modresortchan-service labels: app: modresortchan chart: modresortchan-0.1.0 release: modresort-devchan heritage: Tiller spec: type: NodePort ports: - port: 9080 selector: app: modresortchan release: modresort-devchan tier: frontend Take note of a few elements of the Deployable definition: There are two Deployables defined in the yaml file that wrap the modresort Kubernetes resources One for the modresort deployment One for the modresort service The Deployment and Service specs are specific to the Modresort application deployment to an OpenShift Cluster. For example, the Deployment refers to the location where the Docker image will be pulled, number of replicas, etc For example, the Service refers to the service port, and defines a NodePort for externally accessing the Modresort app in a test environment. Refer to the on-line documentation for details on the construct of the Deployable definition: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_deployables.html","title":"Define the application as a channel"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#create-a-subscription","text":"The subscription to a channel package contains - Application Definition - Placement Rules Definition - Subscription Definition Applications (Application.app.k8s.io) in IBM Multicloud Manager are used for grouping application components. Placement rules (PlacementRule.app.ibm.com) define the target clusters where deployables can be deployed. You can use placement rules to help you facilitate the multi-cluster deployment of your deployables. Placement rules can be referenced by deployables and subscriptions. Subscriptions (Subscription.app.ibm.com) are sets of definitions that identify deployables within channels by using annotations, labels, and versions. The subscription controller can monitor the channel for new or updated deployables, such as an updated Helm release or new Kubernetes deployable object. Then, the controller can download the Kubernetes deployable object or Helm release directly from the source location (Helm repository, object store, or namespace) to the target managed clusters.","title":"Create a subscription"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#create-the-resources-for-application-placementrule-and-subscription","text":"You will create these additional yaml resources for the application in a new folder for the application resources. They will also be deployed to a separate namespace than the Channel and Deployable resources. Note: The yaml files that you will create in this section are also stored in an \u201cInstructor\u201d folder: /home/ibmuser/instructor/modresortapp You can use the snippets below to create the yaml files. Or, you can copy / paste the yaml files from the \u201cInstructor\u201d folder. It is your choice. Create a new directory for your application modresortapp , and change to the new directory: /home/ibmuser/student/modresortapp mkdir /home/ibmuser/student/modresortapp cd /home/ibmuser/student/modresortapp Create the application.yaml which defines the grouping of the application components. Notice the resource kind is \u201cApplication\u201d, and the Component kinds is a \u201cSubscription\u201d. apiVersion: app.k8s.io/v1beta1 kind: Application metadata: name: modresort101-modresortapp namespace: modresort-project labels: app: modresortapp spec: selector: matchExpressions: - key: release operator: In values: - modresort101 componentKinds: - group: app.ibm.com kind: Subscription Refer to the on-line documentation for details on configuring the Application resource: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/app_lifecycle.html> Create placementrules.yaml file. This contains the definition for placement of the components as selected by the user. In the definition you will find placement rule for the modresort component. The selection criteria for where the application will be deployed is based on the \u201c environment \u201d cluster label. The value of \u201c Dev \u201d means the Modresort app will be deployed to the local cluster. A value of \u201c Prod \u201d for the environment label, will result in the Modresort application being deployed to the managed cluster (OCP). apiVersion: app.ibm.com/v1alpha1 kind: PlacementRule metadata: name: modresortapp101-modresortapp namespace: modresort-project labels: app: modresortapp release: modresort101 spec: clusterReplicas: 1 clusterLabels: matchLabels: environment: Prod Refer to the on-line documentation for details on configuring the PlacementRule resource: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_placement_rules.html Create subscription.yaml file. This contains the details for relating the placement rule definition with the application specification. In the definition file, there is a subscription definition for the modresort application component, which references the modresort application channel . Through this subscription, the modresort application will be deployed to the cluster based on the placement rule that is referenced here in the subscription . apiVersion: app.ibm.com/v1alpha1 kind: Subscription metadata: name: modresort101-modresortapp namespace: modresort-project labels: app: modresortapp release: modresort101 spec: channel: modresort-entitlement/modresort-devchan name: \"\" packageFilter: version: \">=1.x\" labelSelector: matchLabels: package: modresort component: main placement: placementRef: name: modresortapp101-modresortapp kind: PlacementRule group: app.ibm.com overrides: - clusterName: \"/\" clusterOverrides: - path: \"metadata.namespace\" value: default Refer to the on-line documentation for details on configuring the Subscription resource: https://www.ibm.com/support/knowledgecenter/en/SSFC4F_1.2.0/mcm/applications/managing_subscriptions.html This completes enabling an existing application with policies so that they can be deployed to any Kubernetes managed cluster via MCM placement policies.","title":"Create the resources for \u201cApplication\u201d, \u201cPlacementRule\u201d, and \u201cSubscription\u201d"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#deploy-the-application-that-is-built","text":"In this section, you will deploy the application components to their respective Kubernetes namespaces, using the yaml files that you created in the previous steps. Launch the command window and log in to the hub OpenShift cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Use the icp-console URL to login to the Multicloud Management console CLI cloudctl login -a https://icp-console.apps.demo.ibmdte.net/ -u admin -p passw0rd -n default As the channel is namespace based, create two namespaces (projects in OpenShift) where one is for the subscription and the other is for the channel . Run the following commands to create the projects in OpenShift oc new-project modresort-project oc new-project modresort-entitlement Deploy the modresortchan application (channel definition) to the modresort - entitlement project oc project modresort-entitlement OUTPUT: Already on project \"modresort-entitlement\" on server \"https://api.demo.ibmdte.net:6443\" cd /home/ibmuser/student oc apply -f modresortchan Output of the above command will be similar to the illustration below. The output shows that Channel and Deployables are created Now deploy the subscription to the \u201c modresort-project\u201d namespace. The subscription is deployed to a different namespace than the channel. Never deploy the subscription to the same namespace as the channel. Hence, deploy the subscription to the other namespace, modresort - project oc project modresort-project OUTPUT: Now using project \"modresort-project\" on server \"https://api.demo.ibmdte.net:6443\" oc apply -f modresortapp Continue to the next section of the lab, which walks you through validating the application deployment.","title":"Deploy the application that is built"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#validate-the-application","text":"As the channel and the subscription are deployed, now you can validate if the application is running. Go to the console: https://icp-console.apps.demo.ibmdte.net/multicloud/welcome using username: admin and password: passw0rd Check the \"Applications view:\" menu -> Manage Applications. You should see the application listed . Note: It may take a couple of moments for the application to be available. Be patient and refresh the page until the application is displayed in the UI. Click on the modresort101-modresortapp application link to get to the application Overview page. a) Then, scroll down to view the application diagram You will see the Resource Topology for the modresort application. Note: It may take a couple of minutes for the application to > deploy to the cluster. When the application is successfully deployed, via the subscription, the pod in the application topology view will have a GREEN icon, as illustrated below. Note: After a couple of minutes, if your pod has a YELLOW icon next to the pod, indicating it is in an unknown state, you will need to troubleshoot your yaml files for correctness, and redeploy the resources. Check your subscriptions to verify they are subscribed and propagated oc get subscription.app.ibm.com --all-namespaces The subscription from the mcm hub should be in the \u201c Propagated\u201d state. Check your subscription on the \u2018 ocp\u2019 node (Prod) cluster to verify the subscription in in the subscribed \u201d state. a) Open a new terminal window b) ssh into the ocp node using the credentials ibmuser / passw0rd ssh ocp -l ibmuser when prompted for the password enter: passw0rd c) Login to the OCP cluster (This is the Prod cluster) oc login -u admin -p passw0rd https://ocp.demo.ibmdte.net:8443 d) Check the status of the subscription in the \u201c default \u201d namespace in the OCP cluster. Note: The subscription.yaml file you created earlier defined the namespace for the Modresort application to be the \u201c default \u201d namespace in the target cluster. oc get subscription.app.ibm.com -n default The subscription in \u201c default \u201d namespace should be in the \u201c Subscribed \u201d state Check the modresort pod state in the \u201c default \u201d namespace in the OCP cluster oc get pods -n default You should see the devchan-modresort-deployment pod running in the \u201c default \u201d namespace Test the modresorts application from the browser Note: Ensure you run the following command from the OCP ssh terminal window a) Use the \u201coc get services -n default\u201d command to get the devchan-modresortchan-service NodePort. oc get services -n default b) From a browser: http://10.0.0.10:\\<NodePort>/resorts Type \u201c exit \u201d to exit from the ssh of the OCP cluster Close the 2 nd terminal window where you connected via ssh to the OCP cluster Congratulations! You have successfully deployed the modresorts application to the local OpenShift Cluster, using the MCM subscription based \u201cPlacementrule\u201d deployment model.","title":"Validate the Application"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#explore-some-valuable-cli-commands-to-view-the-state-of-the-deployed-resources","text":"Using the cli, you can check quite a few things about the deployed resources Ensure you are logged into to the hub openshift cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Using the cli, check the placementrules using the \u201cget placementrules\u201d command: oc get placementrules --all-namespaces Check where the subscription landed, in the status section of the output from the following command: oc get placementrules modresortapp101-modresortapp -n modresort-project -o yaml Enter the command below to check all the deployables . Then you can check the deployables for any errors by using oc logs\u2026. And oc describe commands as needed for troubleshooting. oc get deployables.app.ibm.com --all-namespaces Command to check the registered clusters oc get **cluster**s.**cluster**registry.k8s.io --all-namespaces Similarly, to check any resource, you can use oc api-resources to display all the resources and use any resource. For example: oc api-resources | grep deployable From the above example, to get deployables, you specify the resource including its APIGROUP as illustrated below: oc get deployables.app.ibm.com --all-namespaces Here is the example and output from getting the MCM Application resources oc api-resources | grep application From the above example, to get deployables, you specify the resource including its full API resource name, as illustrated below: oc get applications.app.k8s.io --all-namespaces Congratulations! You are now familiar with several important MCM specific CLI commands that can help you gain insights into the state of your MCM managed applications.","title":"Explore some valuable cli commands to view the state of the deployed resources"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#modify-placement-rules-to-move-the-application-to-the-dev-managed-cluster","text":"The previous portions of the lab deployed the modresort application to the managed cluster labeled as \u201c Prod \u201d. You can work with the placement rule to see how MCM can easily redeploy the application to a different cluster based on selection criteria in the placement rule. In this section, you will modify the placement rule to redeploy the modresort application to the local cluster, labeled Dev \u201d. From the MCM clusters view in the MCM Hub console, you can see all the labels the admin set on the managed clusters. These are the labels on the two managed clusters in the lab environment. Notice the environment label. That is what you defined in the placement rule to deploy the application to the \u201cProd\u201d cluster. The environment label for the local managed cluster is environment=Dev Dev Cluster Prod Cluster","title":"Modify placement rules to move the application to the \"dev\" managed cluster"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#modify-placement-rules-to-deploy-modresorts-to-the-dev-managed-cluster","text":"Return to the Mutlicloud Management Console in the Browser. If you closed the console, you can login again using the URL and credentials below. URL: https://icp-console.apps.demo.ibmdte.net userid: ibmuser password: passw0rd From the menu, navigate to the \u201c Manage Applications\u201d view, where you will see the modresort application listed Click on the modresort101-modresortapp in the list. From the application Overview page, scroll down to the Resource Topology section You can edit the placement policy using the Edit Topology link located in the lower right corner, and then clicking on the placement rules icon as illustrated below. a) Click on the Edit Topology link to view the yaml file. b) Click on the Placement Rules icon, which will take you directly to the placement rules section of the yaml file. c) Place curser on edge of the Editor view, and drag it left, widening the view area. d) Notice the Modresort application is currently deployed to the cluster labeled as \u201c Prod \u201d. e) In the Yaml view, on approximately line #60, modify the environment from Prod to Dev Note: Case Matters \u2013 Upper case \u201cD\u201d f) Click the Update icon to save the changes to the yaml g) Click the Close editor link to close the yaml editor h) After a couple of minutes, the modresort application will be redeployed to the \u201c Dev \u201d managed cluster. Refresh the application view , then verify the Resource Topology section shows that application now deployed to \u201c Dev \u201d, which is the local-cluster cluster. a) The pod, once redeployed to the local cluster, will show in the running state, indicated by a green icon, as illustrated below: It may take a couple of minutes for the application to be redeployed to the local cluster. Login to the local cluster CLI (hub cluster) to run oc commands to verify the application state. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 You can use the following command to see if the modresort pod got deployed and its current state of the pod and subscription, and placement rules oc get pods -n default oc get subscription.app.ibm.com -n default</p> oc get placementrules modresortapp101-modresortapp -n modresort-project -o yaml Note: Output from the above commands are illustrated below: Test the modresorts application from the browser a) Use the oc get services -n default command to get the devchan-modresortchan-service NodePort. oc get services -n default From a browser: http://10.0.0.4:\\<NodePort>/resorts Note: 10.0.0.4 is the \u201cMaster\u201d node of the local-cluster cluster (Dev) Congratulations! You have successfully re-deployed the modresorts application to the \u201cProd\u201d managed OpenShift Cluster, using the MCM subscription based \u201cPlacementrule\u201d deployment model.","title":"Modify Placement Rules to deploy modresorts to the \"Dev\" managed cluster"},{"location":"ITC-22_v1.6-MCM_Application_Deployment_LABGUIDE-tgf/#9-summary","text":"In this lab, you have learned how to enable an existing application definition with channel definition, subscription and placement rules. Congratulations! You have successfully completed the lab \u201cApplication Deployment\u201d","title":"9 Summary"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/","text":"Lab: ITC-26 Cloud Pak for MultiCloud Management - Governance Policy Lab Lab. How to create Policy and enforce it The IBM Cloud Pak\u2122 for Multicloud Management, running on Red Hat\u00ae OpenShift\u00ae, provides consistent visibility, governance and automation from on premises to the edge. Enterprises gain capabilities such as multicluster management, event management, application management and infrastructure management. Enterprises can leverage this IBM Cloud Pak to help increase operational efficiency that is driven by intelligent data analysis and predictive golden signals and gain built-in support for their compliance management. You can also take advantage of the governance, as with this IBM Cloud Pak for Multicloud Management, you can manage your multicloud environments with a consistent set of configuration and security policies across all applications and clusters. This lab walks you through the process of setting up policies and evaluating them. It also serves as examples for creating your own policies. Here is a schematic describing how is the environment provided to create your policies, inform and enforce to clear violations. One hub used also as local cluster and a managed/remote cluster. Objective The objectives of this lab are to: Learn how to create different types of policies, check the state and enforce them to clear violations. Enforce a complex policy using a defined use-case Prerequisites The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands and Kubernetes commands A Management Cluster is available and ready to use An OpenShift cluster that needs to be managed The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information. What is Already Completed Two OpenShift Clusters pre-deployed where one cluster acts as Management Cluster and other acts as managed cluster. On the management cluster, the management software is loaded. The management cluster (named as local-cluster) also configured to manage itself. The managed cluster (named as managed-cluster) The Developer VM is the one you will use to access and work with OCP in this lab. The login credentials for the Developer VM are: User ID: ibmuser Password: passw0rd (this is the same sudo password) Lab Tasks During this lab, you will complete the following tasks: Log in to the Management Cluster from workstation Use the console to create a few governance policies Check the state of the created policies Change the policy enforcement and validate the changes Execute Lab Tasks Log in to the Workstation VM and get started If the VMs are not already started, Launch the six lab VMs by clicking the Play button highlighted in the above picture. After the VMs are started (takes few minutes), click the Developer VM icon to access it. The Developer VM\u2019s Linux Desktop is displayed. You will execute all the lab tasks on this VM. Check the Environment In this task, let\u2019s check the Management environment to ensure it is up and its pods are all running. To check the environment, login to the \u201c Developer \u201d VM using userid: ibmuser and password: passw0rd . The workstation has the required tools oc , cloudctl and helm tools installed Launch a terminal window and login to Management OpenShift Cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Verify all of the managent pods are running in the Management clusters oc get pods -n kube-system | grep -v Running|grep -v Completed oc get pods -n multicluster-endpoint | grep -v Running|grep -v Completed There should NOT be any pods listed in the results. That indicates all of the pods are in the Running state. If there are pods listed, and not running, contact the instructor, as the managent cluster may not be running correctly. Launch a Mozilla Firefox browser, login to the Mutlicloud Management Console, there is already a bookmark configured URL: https://icp-console.apps.demo.ibmdte.net/multicloud/welcome userid: admin password: passw0rd You should see the Multicloud Management Consoles, Welcome page ( read below, if you do not get this page ) NOTE : If you see the incorrect page: \u201c IBMCloud Pak \u201d screen, Rerun the URL link again. This appears to be a bug in our lab environment Reference: Introduction to IBM Multicloud Manager Policies IBM Multicloud Manager enables you to check whether your clusters are operating properly by comparing the current configuration of various resources against your desired state. The system enables you to create compliance templates that can check policies against roles, or pod objects within the clusters. IBM Multicloud Manager supports many types of policies. Network policies : Handled by the SDN (software defined network) resource MCM Policies : handled by the MCM resource definition Object template: Pods Role template: Roles and role bindings Placement Policies : Handled by the MCM Mutation Policies : Handled by the mutation advisor resource Container changes Remediation actions: There are two ways to handle a policy violation in your clusters: Auto remediation by policy controller when \u2018 enforce \u2019 action is specified in policy \u201c Inform \u201d action provides notification that a cluster is out of compliance, but does enforce compliance. Policy controller generates Cloud Event Manager incidents for policy violations MCM Hub generates audit logs to Security information and event management (SIEM) for findings (policy violations are also mapped to findings) Example: Mutation Policy Refer to the documentation for more information: https://www.ibm.com/support/knowledgecenter/SSFC4F_1.2.0/mcm/compliance/compliance_intro.html Create a Namespace enforcement Policy This task creates a policy that ensures that a specified namespace is present in clusters that match the selection criteria. Launch a new Terminal Window, and log in to OpenShift Cluster oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Run the following command to ensure that the environment does not already have a namespace that the \u201cNamespace Policy\u201d will create and enforce. oc get ns | grep k8demo where <k8demo> is the name of the namespace we are searching The previous command should NOT return any results. NOTE: If the namespace already exists, run the following command to delete it: oc delete project k8demo Return to the MCM Web Console https://icp-console.apps.demo.ibmdte.net/multicloud/welcome Login using the following Credentials: User:admin Password: passw0rd Navigate to Menu > Govern risk . Here you can view and create compliance policies for your managed clusters** Click the Policies tab. This view displays the policies that have been created and the dashboard of policy compliance for each cluster. Currently, you do not have any compliance policies created. Click on create policy button. Fill the values as specified in the table below: Field Name Value Name policy-namespace Namespace Choose the \u201c default \u201d namespace Specifications Choose: Namespace-must have namespace \u2018prod\u2019 Note: You will modify the name prod to k8demo. Selecting this will provide a template to have custom namespace policy definition Cluster binding Choose name: \u201c local-cluster \u201d In the yaml file section , on the right, change prod to k8demo . Changing the namespace will change the Policy Specifications to Custom Specifications as below. Notice that the policy is set to \u201c inform \u201d rather than \u201c enforce \u201d. Use the Create button, to create your new policy. If you are not redirected automatically navigate to Menu > Govern risk to return to the Dashboard In few seconds, the policy controller will check if the namespace k8demo is present and provides information regarding the current compliance of the policies. Remember, you didn\u2019t enforce this policy. Instead we specified inform. As such, the Governance and risk view displays a policy violation in our cluster, as illustrated below. Use the Cluster Violations link to find which cluster is violating the policy. The link is located to the far right of the screen, as illustrated below. The local-cluster cluster is in violation of the policy which requires a namespace called \u201c k8demo \u201d to exist. The local-cluster cluster is our cluster, and the same cluster that verified \u201ck8demo\u201d namespace does not exist. Hence it shows that there is no namespace k8demo in the cluster. Verify the k8demo namespace still does not exist. oc get project | grep k8demo There should NOT be a namespace named k8demo listed, which indicates the policy did not ENFORCE it to be created. Change the \u201cpolicy-namespace\u201d policy to be enforced When a policy is in \u201c enforce \u201d mode, the namespace will automatically be created, if it does not exist, thereby enforcing the cluster into compliance. a) In the policies view, select POLICY VIOLATIONS link, as shown below b) Then, select the policy named \u201c policy-namespace \u201d and go to YAML view c) Click on the button to go into edit mode. d) Change the value of \u201c remediationAction: inform \u201d to \u201c remediationAction: enforce \u201d e) Click the to submit the change Select the policy-namespace link. A few seconds later, the policy violation will be gone You also can validate the same from the Violations view. Run the following command to ensure that the k8demo namespace has been created in the cluster. oc get ns | grep k8demo Try deleting the namespace and check how is being created automatically again. oc delete project k8demo Create a Network Policy to enforce on a namespace In this task, you create a network policy just for informational purpose. Then modify it to enforce the policy. In this scenario, the Network Policy is used to control (block) network traffic from other pods. You will perform the following tasks: Create a project (namespace) for this policy Create NetworkPolicy targeting the new namespace Check if the network policy is configured correctly Let\u2019s create a new project to enforce a Network Deny Policy to disable network traffic from pods a) login to Management OpenShift Cluster oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 b) Create a new project with name denypolicy oc new-project denypolicy Return to the MCM console . Then go to Menu -> Governance and Risk This launches the policy management view. If there are no policy, no policies will be listed. Select the button to create a new policy Configure the new network policy according to the table below Field Name Value Name of the policy policy-network Namespaces Choose: \u201cdenypolicy\u201d Specifications Choose: \u201cNetworkpolicy-deny network request\u201d Cluster binding Choose name: \u201c local-cluster \u201d In the right side section (YAML), change the namespace from include: [\u201cdefault\u201d] to include: [\u201cdenypolicy\u201d] Select the button which returns console to Policies view. After few seconds, you will find that there is a cluster violation on the network policy, policy-network Select the CLUSTER VIOLATIONS link to view which cluster is violating this policy. As, this policy is configured for informational purpose, the violation is just shown. It is not rectified. Change the \u201c policy-network \u201d policy to be enforced a) In the policies view, select POLICY VIOLATIONS b) Then, select the policy named \u201c policy-network \u201d and go to YAML view c) Click on the button to go into edit mode. d) Change the value of \u201c remediationAction: inform \u201d to \u201c remediationAction: enforce \u201d e) Click the to submit the change Go back to the policy-network view and see the violation is cleared You can validate the network policy that is created on the namespace \u201c denypolicy \u201d. a) Using the CLI, run the following command to get the network policies for the namespace oc -n denypolicy get networkpolicies Create Policy that limits memory range for a namespace In this task, you will create a policy where it enforces limits for memory range in a given namespace. Return to the MCM console . Then go to Menu -> Governance and Risk . This launches the policy management view. If there are no policy, no policies will be listed. Select the button to create a new policy Configure the new policy, enforcing quota limits, according to the table below: Field Name Value Name policy-quotalimit Specifications Choose: Limitrange-limit memory usage Namespaces Choose: k8demo Cluster bindings Choose name: \u201c local-cluster \u201d Enforce if supported Check the box In the right-side section ( YAML ), change the namespace where the quota needs to be created. a) Change from include: [\u201cdefault\u201d] to include: [\u201ck8demo\u201d] , which is a namespace we created earlier in the lab, by policy enforcement. As an enhancement, you will modify the YAML template to add the quota for the namespace, like max and min values for CPU and Memory along with a default range. Modify the YAML to include the limits that were mentioned in the previous step. a) The YAML file section as highlighted below before and after is shown below. Ensure the whitespaces match the syntax, as illustrated below: Before: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container After: limits: - type: \"Pod\" max: cpu: \"2\" memory: \"1Gi\" min: cpu: \"200m\" memory: \"6Mi\" - type: \"Container\" max: cpu: \"2\" memory: \"1Gi\" min: <br> cpu: \"100m\" memory: \"4Mi\" default: cpu: \"300m\" Review the YAML before you create the policy Select the button to create the policy Validate if the policy is enforced. Validate the quota is created on the k8demo namespace oc -n k8demo get limits oc -n k8demo get limits -o yaml The resource limits for the k8sdemo namespace are shown below. Create Policy that a pod must exist in a given namespace In this task, you will create a policy where it enforces that a pod is present in a given namespace. Return to the MCM console , and go to Menu -> Governance and Risk . This launches the policy management view. If there are no policy, no policies will be listed. FYI: You may notice HIGH SEVEERITY SECURITY FINDINGS listed in the Governance and Risks section of the MCM console. You can ignore these findings during the lab. MCM records and maintains severity security findings based on the policies you have created. In this lab, the network security policy violation was a high severity security finding. By default, MCM lists these findings, and will automatically be removed from the view after some pre-determined time in the future. Select the button to create a new policy Configure the new policy, requiring pod be present, according to the table below Field Name Value Name policy-pod Specifications Choose: Pod-nginx pod must exist Namespaces Choose: k8demo Cluster bindings Choose name: \u201c local-cluster \u201d Enforce if supported Check the box In the right side section (YAML), change the namespace where the pod needs to be created. a) Change from include: [\u201cdefault\u201d] to include: [\u201ck8demo\u201d] , which is a namespace we created earlier in the lab, by policy enforcement. b) Review the policy settings, as illustrated below Select the button to create the policy Select the policy-pod to launch the policy view. You will find initially it will be in violation and will quickly turn to green as the enforcement kicks in and creates the pod. Known issue: False positive violation when enforcing policy to create a pod #2286 This will be fixed in the next iteration From the CLI validate if the pod is running in the k8demo namespace oc -n k8demo get pods Summary In this lab, you have learned how to create policies both for informational and enforcement purposes. You also learnt how to modify the existing policies to change from inform to enforce mode. Congratulations! You have successfully completed the lab \u201cPolicy Enforcement\u201d","title":"***Lab: ITC-26***"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#lab-itc-26","text":"","title":"Lab: ITC-26"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#cloud-pak-for-multicloud-management-governance-policy-lab","text":"","title":"Cloud Pak for MultiCloud Management - Governance Policy Lab"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#lab-how-to-create-policy-and-enforce-it","text":"The IBM Cloud Pak\u2122 for Multicloud Management, running on Red Hat\u00ae OpenShift\u00ae, provides consistent visibility, governance and automation from on premises to the edge. Enterprises gain capabilities such as multicluster management, event management, application management and infrastructure management. Enterprises can leverage this IBM Cloud Pak to help increase operational efficiency that is driven by intelligent data analysis and predictive golden signals and gain built-in support for their compliance management. You can also take advantage of the governance, as with this IBM Cloud Pak for Multicloud Management, you can manage your multicloud environments with a consistent set of configuration and security policies across all applications and clusters. This lab walks you through the process of setting up policies and evaluating them. It also serves as examples for creating your own policies. Here is a schematic describing how is the environment provided to create your policies, inform and enforce to clear violations. One hub used also as local cluster and a managed/remote cluster.","title":"Lab. How to create Policy and enforce it"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#objective","text":"The objectives of this lab are to: Learn how to create different types of policies, check the state and enforce them to clear violations. Enforce a complex policy using a defined use-case","title":"Objective"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#prerequisites","text":"The following prerequisites must be completed prior to beginning this lab: Familiarity with basic Linux commands and Kubernetes commands A Management Cluster is available and ready to use An OpenShift cluster that needs to be managed The following symbols appear in this document at places where additional guidance is available. Icon Purpose Explanation Important! This symbol calls attention to a particular step or command. For example, it might alert you to type a command carefully because it is case sensitive. Information This symbol indicates information that might not be necessary to complete a step, but is helpful or good to know. Trouble-shooting This symbol indicates that you can fix a specific problem by completing the associated troubleshooting information.","title":"Prerequisites"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#what-is-already-completed","text":"Two OpenShift Clusters pre-deployed where one cluster acts as Management Cluster and other acts as managed cluster. On the management cluster, the management software is loaded. The management cluster (named as local-cluster) also configured to manage itself. The managed cluster (named as managed-cluster) The Developer VM is the one you will use to access and work with OCP in this lab. The login credentials for the Developer VM are: User ID: ibmuser Password: passw0rd (this is the same sudo password)","title":"What is Already Completed"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#lab-tasks","text":"During this lab, you will complete the following tasks: Log in to the Management Cluster from workstation Use the console to create a few governance policies Check the state of the created policies Change the policy enforcement and validate the changes","title":"Lab Tasks"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#execute-lab-tasks","text":"","title":"Execute Lab Tasks"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#log-in-to-the-workstation-vm-and-get-started","text":"If the VMs are not already started, Launch the six lab VMs by clicking the Play button highlighted in the above picture. After the VMs are started (takes few minutes), click the Developer VM icon to access it. The Developer VM\u2019s Linux Desktop is displayed. You will execute all the lab tasks on this VM.","title":"Log in to the Workstation VM and get started"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#check-the-environment","text":"In this task, let\u2019s check the Management environment to ensure it is up and its pods are all running. To check the environment, login to the \u201c Developer \u201d VM using userid: ibmuser and password: passw0rd . The workstation has the required tools oc , cloudctl and helm tools installed Launch a terminal window and login to Management OpenShift Cluster. oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Verify all of the managent pods are running in the Management clusters oc get pods -n kube-system | grep -v Running|grep -v Completed oc get pods -n multicluster-endpoint | grep -v Running|grep -v Completed There should NOT be any pods listed in the results. That indicates all of the pods are in the Running state. If there are pods listed, and not running, contact the instructor, as the managent cluster may not be running correctly. Launch a Mozilla Firefox browser, login to the Mutlicloud Management Console, there is already a bookmark configured URL: https://icp-console.apps.demo.ibmdte.net/multicloud/welcome userid: admin password: passw0rd You should see the Multicloud Management Consoles, Welcome page ( read below, if you do not get this page ) NOTE : If you see the incorrect page: \u201c IBMCloud Pak \u201d screen, Rerun the URL link again. This appears to be a bug in our lab environment","title":"Check the Environment"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#reference-introduction-to-ibm-multicloud-manager-policies","text":"IBM Multicloud Manager enables you to check whether your clusters are operating properly by comparing the current configuration of various resources against your desired state. The system enables you to create compliance templates that can check policies against roles, or pod objects within the clusters. IBM Multicloud Manager supports many types of policies. Network policies : Handled by the SDN (software defined network) resource MCM Policies : handled by the MCM resource definition Object template: Pods Role template: Roles and role bindings Placement Policies : Handled by the MCM Mutation Policies : Handled by the mutation advisor resource Container changes Remediation actions: There are two ways to handle a policy violation in your clusters: Auto remediation by policy controller when \u2018 enforce \u2019 action is specified in policy \u201c Inform \u201d action provides notification that a cluster is out of compliance, but does enforce compliance. Policy controller generates Cloud Event Manager incidents for policy violations MCM Hub generates audit logs to Security information and event management (SIEM) for findings (policy violations are also mapped to findings) Example: Mutation Policy Refer to the documentation for more information: https://www.ibm.com/support/knowledgecenter/SSFC4F_1.2.0/mcm/compliance/compliance_intro.html","title":"Reference: Introduction to IBM Multicloud Manager Policies"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#create-a-namespace-enforcement-policy","text":"This task creates a policy that ensures that a specified namespace is present in clusters that match the selection criteria. Launch a new Terminal Window, and log in to OpenShift Cluster oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 Run the following command to ensure that the environment does not already have a namespace that the \u201cNamespace Policy\u201d will create and enforce. oc get ns | grep k8demo where <k8demo> is the name of the namespace we are searching The previous command should NOT return any results. NOTE: If the namespace already exists, run the following command to delete it: oc delete project k8demo Return to the MCM Web Console https://icp-console.apps.demo.ibmdte.net/multicloud/welcome Login using the following Credentials: User:admin Password: passw0rd Navigate to Menu > Govern risk . Here you can view and create compliance policies for your managed clusters** Click the Policies tab. This view displays the policies that have been created and the dashboard of policy compliance for each cluster. Currently, you do not have any compliance policies created. Click on create policy button. Fill the values as specified in the table below: Field Name Value Name policy-namespace Namespace Choose the \u201c default \u201d namespace Specifications Choose: Namespace-must have namespace \u2018prod\u2019 Note: You will modify the name prod to k8demo. Selecting this will provide a template to have custom namespace policy definition Cluster binding Choose name: \u201c local-cluster \u201d In the yaml file section , on the right, change prod to k8demo . Changing the namespace will change the Policy Specifications to Custom Specifications as below. Notice that the policy is set to \u201c inform \u201d rather than \u201c enforce \u201d. Use the Create button, to create your new policy. If you are not redirected automatically navigate to Menu > Govern risk to return to the Dashboard In few seconds, the policy controller will check if the namespace k8demo is present and provides information regarding the current compliance of the policies. Remember, you didn\u2019t enforce this policy. Instead we specified inform. As such, the Governance and risk view displays a policy violation in our cluster, as illustrated below. Use the Cluster Violations link to find which cluster is violating the policy. The link is located to the far right of the screen, as illustrated below. The local-cluster cluster is in violation of the policy which requires a namespace called \u201c k8demo \u201d to exist. The local-cluster cluster is our cluster, and the same cluster that verified \u201ck8demo\u201d namespace does not exist. Hence it shows that there is no namespace k8demo in the cluster. Verify the k8demo namespace still does not exist. oc get project | grep k8demo There should NOT be a namespace named k8demo listed, which indicates the policy did not ENFORCE it to be created. Change the \u201cpolicy-namespace\u201d policy to be enforced When a policy is in \u201c enforce \u201d mode, the namespace will automatically be created, if it does not exist, thereby enforcing the cluster into compliance. a) In the policies view, select POLICY VIOLATIONS link, as shown below b) Then, select the policy named \u201c policy-namespace \u201d and go to YAML view c) Click on the button to go into edit mode. d) Change the value of \u201c remediationAction: inform \u201d to \u201c remediationAction: enforce \u201d e) Click the to submit the change Select the policy-namespace link. A few seconds later, the policy violation will be gone You also can validate the same from the Violations view. Run the following command to ensure that the k8demo namespace has been created in the cluster. oc get ns | grep k8demo Try deleting the namespace and check how is being created automatically again. oc delete project k8demo","title":"Create a Namespace enforcement Policy"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#create-a-network-policy-to-enforce-on-a-namespace","text":"In this task, you create a network policy just for informational purpose. Then modify it to enforce the policy. In this scenario, the Network Policy is used to control (block) network traffic from other pods. You will perform the following tasks: Create a project (namespace) for this policy Create NetworkPolicy targeting the new namespace Check if the network policy is configured correctly Let\u2019s create a new project to enforce a Network Deny Policy to disable network traffic from pods a) login to Management OpenShift Cluster oc login -u admin -p passw0rd https://api.demo.ibmdte.net:6443 b) Create a new project with name denypolicy oc new-project denypolicy Return to the MCM console . Then go to Menu -> Governance and Risk This launches the policy management view. If there are no policy, no policies will be listed. Select the button to create a new policy Configure the new network policy according to the table below Field Name Value Name of the policy policy-network Namespaces Choose: \u201cdenypolicy\u201d Specifications Choose: \u201cNetworkpolicy-deny network request\u201d Cluster binding Choose name: \u201c local-cluster \u201d In the right side section (YAML), change the namespace from include: [\u201cdefault\u201d] to include: [\u201cdenypolicy\u201d] Select the button which returns console to Policies view. After few seconds, you will find that there is a cluster violation on the network policy, policy-network Select the CLUSTER VIOLATIONS link to view which cluster is violating this policy. As, this policy is configured for informational purpose, the violation is just shown. It is not rectified. Change the \u201c policy-network \u201d policy to be enforced a) In the policies view, select POLICY VIOLATIONS b) Then, select the policy named \u201c policy-network \u201d and go to YAML view c) Click on the button to go into edit mode. d) Change the value of \u201c remediationAction: inform \u201d to \u201c remediationAction: enforce \u201d e) Click the to submit the change Go back to the policy-network view and see the violation is cleared You can validate the network policy that is created on the namespace \u201c denypolicy \u201d. a) Using the CLI, run the following command to get the network policies for the namespace oc -n denypolicy get networkpolicies","title":"Create a Network Policy to enforce on a namespace"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#create-policy-that-limits-memory-range-for-a-namespace","text":"In this task, you will create a policy where it enforces limits for memory range in a given namespace. Return to the MCM console . Then go to Menu -> Governance and Risk . This launches the policy management view. If there are no policy, no policies will be listed. Select the button to create a new policy Configure the new policy, enforcing quota limits, according to the table below: Field Name Value Name policy-quotalimit Specifications Choose: Limitrange-limit memory usage Namespaces Choose: k8demo Cluster bindings Choose name: \u201c local-cluster \u201d Enforce if supported Check the box In the right-side section ( YAML ), change the namespace where the quota needs to be created. a) Change from include: [\u201cdefault\u201d] to include: [\u201ck8demo\u201d] , which is a namespace we created earlier in the lab, by policy enforcement. As an enhancement, you will modify the YAML template to add the quota for the namespace, like max and min values for CPU and Memory along with a default range. Modify the YAML to include the limits that were mentioned in the previous step. a) The YAML file section as highlighted below before and after is shown below. Ensure the whitespaces match the syntax, as illustrated below: Before: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container After: limits: - type: \"Pod\" max: cpu: \"2\" memory: \"1Gi\" min: cpu: \"200m\" memory: \"6Mi\" - type: \"Container\" max: cpu: \"2\" memory: \"1Gi\" min: <br> cpu: \"100m\" memory: \"4Mi\" default: cpu: \"300m\" Review the YAML before you create the policy Select the button to create the policy Validate if the policy is enforced. Validate the quota is created on the k8demo namespace oc -n k8demo get limits oc -n k8demo get limits -o yaml The resource limits for the k8sdemo namespace are shown below.","title":"Create Policy that limits memory range for a namespace"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#create-policy-that-a-pod-must-exist-in-a-given-namespace","text":"In this task, you will create a policy where it enforces that a pod is present in a given namespace. Return to the MCM console , and go to Menu -> Governance and Risk . This launches the policy management view. If there are no policy, no policies will be listed. FYI: You may notice HIGH SEVEERITY SECURITY FINDINGS listed in the Governance and Risks section of the MCM console. You can ignore these findings during the lab. MCM records and maintains severity security findings based on the policies you have created. In this lab, the network security policy violation was a high severity security finding. By default, MCM lists these findings, and will automatically be removed from the view after some pre-determined time in the future. Select the button to create a new policy Configure the new policy, requiring pod be present, according to the table below Field Name Value Name policy-pod Specifications Choose: Pod-nginx pod must exist Namespaces Choose: k8demo Cluster bindings Choose name: \u201c local-cluster \u201d Enforce if supported Check the box In the right side section (YAML), change the namespace where the pod needs to be created. a) Change from include: [\u201cdefault\u201d] to include: [\u201ck8demo\u201d] , which is a namespace we created earlier in the lab, by policy enforcement. b) Review the policy settings, as illustrated below Select the button to create the policy Select the policy-pod to launch the policy view. You will find initially it will be in violation and will quickly turn to green as the enforcement kicks in and creates the pod.","title":"Create Policy that a pod must exist in a given namespace"},{"location":"ITC-26_v1.1-MCM_Policy_Lab_LABGUIDE-tgf/#summary","text":"In this lab, you have learned how to create policies both for informational and enforcement purposes. You also learnt how to modify the existing policies to change from inform to enforce mode. Congratulations! You have successfully completed the lab \u201cPolicy Enforcement\u201d","title":"Summary"},{"location":"OLD-index/","text":"PYRK8S Bootcamp - South Africa IBM Client Center, 90 Grayston Drive, Sandton, Johannesburg Oct 28 - Nov 1 Time: 9:30am-5:30pm daily Slack Channel for class communications: #pyrk8s-south-africa (In the IBM Cloud team) Objective for F2F Training Apply the vendor-agnostic principles from the virtual training to IBM (and Red Hat) Kubernetes solutions. Using this knowledge, attendees will create a MVP project and team to execute that project remotely. Agenda The agenda is spread over 5 days onsite. During the face to face training teams will discuss and architect an MVP solution which will be executed later. Presentations PyRK8's material password: pyrk8sibm Hands on Activities This bootcamp contains significant hands on learning. From the navigation menu , select the Daily Agenda and Activities item to view the daily agenda. There, you will also find the links to the hands-on lab guides and Skytap environments that are used for that specific day.","title":"PYRK8S Bootcamp - South Africa"},{"location":"OLD-index/#pyrk8s-bootcamp-south-africa","text":"IBM Client Center, 90 Grayston Drive, Sandton, Johannesburg Oct 28 - Nov 1 Time: 9:30am-5:30pm daily Slack Channel for class communications: #pyrk8s-south-africa (In the IBM Cloud team) Objective for F2F Training Apply the vendor-agnostic principles from the virtual training to IBM (and Red Hat) Kubernetes solutions. Using this knowledge, attendees will create a MVP project and team to execute that project remotely. Agenda The agenda is spread over 5 days onsite. During the face to face training teams will discuss and architect an MVP solution which will be executed later. Presentations PyRK8's material password: pyrk8sibm Hands on Activities This bootcamp contains significant hands on learning. From the navigation menu , select the Daily Agenda and Activities item to view the daily agenda. There, you will also find the links to the hands-on lab guides and Skytap environments that are used for that specific day.","title":"PYRK8S Bootcamp - South Africa"},{"location":"appmod/","text":"Lab Content: Application Modernization Lab Guides ITC-19: Evaluating Java Applications for Modernization ITC-20: Modernize WebSphere app to WAS Base Container in IBM Cloud Pak for Applications ITC-21: Create and Deploy a Cloud Native App to IBM Cloud Pak for Applications ITC-22: Enable policy driven deployments using BM Multicloud manager ITC-26: Create and enforce multi-cluster Governance policies with IBM Multicloud Manager","title":"Application Modernization Lab Guides"},{"location":"appmod/#lab-content-application-modernization-lab-guides","text":"ITC-19: Evaluating Java Applications for Modernization ITC-20: Modernize WebSphere app to WAS Base Container in IBM Cloud Pak for Applications ITC-21: Create and Deploy a Cloud Native App to IBM Cloud Pak for Applications ITC-22: Enable policy driven deployments using BM Multicloud manager ITC-26: Create and enforce multi-cluster Governance policies with IBM Multicloud Manager","title":"Lab Content: Application Modernization Lab Guides"},{"location":"lab-index/","text":"TechCon2020 Labs TechCon 2020 Repo for Dallas/FW Cloud Pak for Integration Labs Cloud Pak for Integration Product Tour Create account with Cloud Pak for Integration API-led Integration with Cloud Pak for Integration APIC (v2018) Dev Jam Lab 1 - Create and Secure an API APIC (v2018) Dev Jam - Lab 2 - The Developer Portal Experience APIC (v2018) Dev Jam Lab 3 - Add OAuth Security to your API APIC (v2018) Dev Jam Lab 4 - Use Lifecycle Controls to Version your API APIC (v2018) Dev Jam Lab 5 - Advanced API Assembly APIC (v2018) Dev Jam - Lab 6 - Working with API Products APIC (v2018) Dev Jam - Lab 7 - The Consumer Experience Deploy an App integration and expose it securely as APIs Connect an App Integration with Message Queue using Cloud Pak for Integration Connect IBM MQ with IBM Event Streams using the Kafka source connector Try IBM Event Streams in a demo environment Integrate Kafka with business Applications to create new responsive experiences MQ High Availability with Replicated Data Queue Managers (RDQM) - HA MQ High Availability with Replicated Data Queue Managers (RDQM) - DR MQ Appliance DP Appliance Hybrid Cloud Data Movement Basics with Aspera on CP4I Blockchain Labs IBM Blockchain Platform Hands-On: An overview to the IBM VS Code Extension for Blockchian. IBM Blockchain Platform Hands-On: Connecting and modifying an existing network using the IBM VS Code Extension for Blockchian. Build a Network on the IBM Blockchain Platform Application Modernization and Multi-Cloud Management Labs Accelerate your modernizing journey for existing WebSphere applications App Modernization using WAS Base Containers on RedHat OpenShift Create and Deploy a Cloud Native App to IBM Cloud pak for Applications (ICPA) Enable policy driven deployments of applications using IBM Multicloud Manager Create and enforce multi-cluster Governance policies with IBM Multicloud Manager Ansible with CloudForms and Cloud Automation Manager Edge Intro and Lab","title":"Index of labs"},{"location":"lab-index/#techcon2020-labs","text":"TechCon 2020 Repo for Dallas/FW","title":"TechCon2020 Labs"},{"location":"lab-index/#cloud-pak-for-integration-labs","text":"Cloud Pak for Integration Product Tour Create account with Cloud Pak for Integration API-led Integration with Cloud Pak for Integration APIC (v2018) Dev Jam Lab 1 - Create and Secure an API APIC (v2018) Dev Jam - Lab 2 - The Developer Portal Experience APIC (v2018) Dev Jam Lab 3 - Add OAuth Security to your API APIC (v2018) Dev Jam Lab 4 - Use Lifecycle Controls to Version your API APIC (v2018) Dev Jam Lab 5 - Advanced API Assembly APIC (v2018) Dev Jam - Lab 6 - Working with API Products APIC (v2018) Dev Jam - Lab 7 - The Consumer Experience Deploy an App integration and expose it securely as APIs Connect an App Integration with Message Queue using Cloud Pak for Integration Connect IBM MQ with IBM Event Streams using the Kafka source connector Try IBM Event Streams in a demo environment Integrate Kafka with business Applications to create new responsive experiences MQ High Availability with Replicated Data Queue Managers (RDQM) - HA MQ High Availability with Replicated Data Queue Managers (RDQM) - DR MQ Appliance DP Appliance Hybrid Cloud Data Movement Basics with Aspera on CP4I","title":"Cloud Pak for Integration Labs"},{"location":"lab-index/#blockchain-labs","text":"IBM Blockchain Platform Hands-On: An overview to the IBM VS Code Extension for Blockchian. IBM Blockchain Platform Hands-On: Connecting and modifying an existing network using the IBM VS Code Extension for Blockchian. Build a Network on the IBM Blockchain Platform","title":"Blockchain Labs"},{"location":"lab-index/#application-modernization-and-multi-cloud-management-labs","text":"Accelerate your modernizing journey for existing WebSphere applications App Modernization using WAS Base Containers on RedHat OpenShift Create and Deploy a Cloud Native App to IBM Cloud pak for Applications (ICPA) Enable policy driven deployments of applications using IBM Multicloud Manager Create and enforce multi-cluster Governance policies with IBM Multicloud Manager Ansible with CloudForms and Cloud Automation Manager Edge Intro and Lab","title":"Application Modernization and Multi-Cloud Management Labs"}]}